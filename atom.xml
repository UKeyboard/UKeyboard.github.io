<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jun&#39;s Blog</title>
  
  <subtitle>山高水长</subtitle>
  <link href="https://ukeyboard.github.io/atom.xml" rel="self"/>
  
  <link href="https://ukeyboard.github.io/"/>
  <updated>2021-11-28T04:22:56.715Z</updated>
  <id>https://ukeyboard.github.io/</id>
  
  <author>
    <name>Jun He</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Elasticsearch报org.elasticsearch.bootstrap.StartupException: ElasticsearchException错误</title>
    <link href="https://ukeyboard.github.io/2021/11/28/Elasticsearch%E6%8A%A5org-elasticsearch-bootstrap-StartupException-ElasticsearchException%E9%94%99%E8%AF%AF/"/>
    <id>https://ukeyboard.github.io/2021/11/28/Elasticsearch%E6%8A%A5org-elasticsearch-bootstrap-StartupException-ElasticsearchException%E9%94%99%E8%AF%AF/</id>
    <published>2021-11-28T03:50:24.000Z</published>
    <updated>2021-11-28T04:22:56.715Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Elasticsearch报org-elasticsearch-bootstrap-StartupException-ElasticsearchException错误"><a href="#Elasticsearch报org-elasticsearch-bootstrap-StartupException-ElasticsearchException错误" class="headerlink" title="Elasticsearch报org.elasticsearch.bootstrap.StartupException:ElasticsearchException错误"></a>Elasticsearch报<code>org.elasticsearch.bootstrap.StartupException:ElasticsearchException</code>错误</h2><h3 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h3><p>在部署Elasticsearch时，我们通常要在配置文件<code>/usr/share/elasticsearch/config/elasticsearch.yml</code>中配置数据存储路径，如下：</p><pre><code class="yaml">......path.data: /usr/share/elasticsearch/data......</code></pre><p>但是，如果在运行Elasticsearch时用户没有该数据目录的读写权限就会导致Elasticsearch报<code>org.elasticsearch.bootstrap.StartupException:ElasticsearchException</code>错误。</p><p>这类错误通常在使用<code>docker</code>或在Kubernetes中部署Elasticsearch时会比较常遇到，主要是因问通过这两种方式部署Elasticsearch时，Elasticsearch在容器内以<code>elasticsearch (uid：1000)</code>运行，如果挂在的外部目录在所有用户ID不是1000或者其他用户对该目录没有访问权限，就会导致上述权限问题。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>解决这类问题最直接的方法是在宿主机（host）上直接将目录的访问权限设置为<code>777</code>或者将目录的所有者切换成UID=1000的用户，如下：</p><pre><code class="shell"># 修改目录的访问权限chmod -R 777 /path/to/folder# orchmod -R a+rw /path/to/folder# 修改目录所有者chown -R 1000 /path/to/folder</code></pre><p>如果是在k8s中遇到这个问题还可以用<code>initContainers</code>在启动Pod前判断目录的所有者或权限并按需进行修改，使得文件或目录在用于Pod前拥有适当的权限设置。下面是我在实际应用中使用的方法：</p><pre><code class="yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: elasticsearch  labels:    role: deploy    target: elasticsearchspec:  replicas: 1 # currently only run in standalone mode  selector:    matchLabels:      app: elasticsearch  template:    metadata:      labels:        app: elasticsearch        role: pod    spec:      volumes:        - name: elasticsearch-yml          configMap:            name: elasticsearch            items:              - key: elasticsearch.yml                path: elasticsearch.yml        - name: data          persistentVolumeClaim:            claimName: elasticsearch      containers:      - name: elasticsearch        image: registry.example.com/elasticsearch:7.15.2         resources:            limits:              ...              ...              ...        ports:        ...        ...        ...        volumeMounts:        - name: data          mountPath: /usr/share/elasticsearch/data        - name: elasticsearch-yml          mountPath: /usr/share/elasticsearch/config/elasticsearch.yml          subPath: elasticsearch.yml        env:          - name: node.name          ...          ...          ...          - name: ES_JAVA_OPTS            value: &quot;-Xms512m -Xmx512m&quot;       initContainers:      - name: fix-permissions        image: registry.example.com/busybox:latest        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;if [ `stat -c &#39;%u&#39; /usr/share/elasticsearch/data`x != &#39;1000x&#39; ]; then chown -R 1000:1000 /usr/share/elasticsearch/data; fi&quot;]        # securityContext:        #   privileged: true        volumeMounts:        - name: data          mountPath: /usr/share/elasticsearch/data</code></pre><p>可以看到，在启动Pod前<code>initContainers</code>尝试判断目录所有者是不是UID=1000用户，如果不是则将其所有者修改问1000:1000。</p><p>注意：如果目录是从宿主机挂载的，比如通过NFS等形式，<code>initContainers</code>在容器内对目录权限的修改会同步反应在宿主机目录上。在宿主机上切换到相应的目录，可以发现目录所有者在宿主机上的UID也是1000.</p>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h2 id=&quot;Elasticsearch报org-elasticsearch-bootstrap-StartupException-ElasticsearchException错误&quot;&gt;&lt;a</summary>
        
      
    
    
    
    <category term="Bugfix" scheme="https://ukeyboard.github.io/categories/Bugfix/"/>
    
    
    <category term="Elasticsearch" scheme="https://ukeyboard.github.io/tags/Elasticsearch/"/>
    
    <category term="Issue" scheme="https://ukeyboard.github.io/tags/Issue/"/>
    
  </entry>
  
  <entry>
    <title>关于k8s emptyDir的几个知识点</title>
    <link href="https://ukeyboard.github.io/2021/11/28/%E5%85%B3%E4%BA%8Ek8s-emptyDir-%E7%9A%84%E5%87%A0%E4%B8%AA%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>https://ukeyboard.github.io/2021/11/28/%E5%85%B3%E4%BA%8Ek8s-emptyDir-%E7%9A%84%E5%87%A0%E4%B8%AA%E7%9F%A5%E8%AF%86%E7%82%B9/</id>
    <published>2021-11-28T03:36:26.000Z</published>
    <updated>2021-11-28T03:44:02.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于k8s-emptyDir的几个知识点"><a href="#关于k8s-emptyDir的几个知识点" class="headerlink" title="关于k8s emptyDir的几个知识点"></a>关于k8s emptyDir的几个知识点</h2><p>下面是关于Kubernetes （简称k8s）中<code>emptyDir</code> volume 的几个知识点，转载自：<a href="https://www.devopsschool.com/blog/kubernetes-volume-emptydir-explained-with-examples/">Kubernetes volume emptyDir explained with examples</a>。</p><blockquote><p>Here are the following facts for emptyDir storage type in Kubernetes</p><ul><li>An emptyDir volume is first created when a Pod is assigned to a Node and initially its empty</li><li>A Volume of type emptyDir that lasts for the life of the Pod, even if the Container terminates and restarts.</li><li>If a container in a Pod crashes the emptyDir content is unaffected.</li><li>All containers in a Pod share use of the emptyDir volume .</li><li>Each container can independently mount the emptyDir at the same / or different path.</li><li>Using emptyDir, The Kubelet will create the directory in the container, but not mount any storage.</li><li>Containers in the Pod can all read/write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each Container.</li><li>When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever along with the container.</li><li>A Container crashing does NOT remove a Pod from a node, so the data in an emptyDir volume is safe across Container crashes.</li><li>By default, emptyDir volumes are stored on whatever medium is backing the node – that might be disk or SSD or network storage.</li><li>You can set the emptyDir.medium field to “Memory” to tell Kubernetes to mount a tmpfs (RAM-backed filesystem) for you instead.</li><li>The location should of emptyDir should be in <code>/var/lib/kubelet/pods/&#123;podid&#125;/volumes/kubernetes.io~empty-dir/</code> on the given node where your pod is running.</li></ul></blockquote>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h2 id=&quot;关于k8s-emptyDir的几个知识点&quot;&gt;&lt;a href=&quot;#关于k8s-emptyDir的几个知识点&quot; class=&quot;headerlink&quot; title=&quot;关于k8s emptyDir的几个知识点&quot;&gt;&lt;/a&gt;关于k8s</summary>
        
      
    
    
    
    <category term="wiki" scheme="https://ukeyboard.github.io/categories/wiki/"/>
    
    
    <category term="Linux" scheme="https://ukeyboard.github.io/tags/Linux/"/>
    
    <category term="Kubernetes cluster" scheme="https://ukeyboard.github.io/tags/Kubernetes-cluster/"/>
    
  </entry>
  
  <entry>
    <title>mount -t nfs fails with Stale file handle 的解决方法</title>
    <link href="https://ukeyboard.github.io/2021/11/26/mount-nfs-fails-with-stale-file-handle/"/>
    <id>https://ukeyboard.github.io/2021/11/26/mount-nfs-fails-with-stale-file-handle/</id>
    <published>2021-11-26T14:52:42.000Z</published>
    <updated>2021-11-26T15:21:31.913Z</updated>
    
    <content type="html"><![CDATA[<h2 id="mount-t-nfs-fails-with-Stale-file-handle-的解决方法"><a href="#mount-t-nfs-fails-with-Stale-file-handle-的解决方法" class="headerlink" title="mount -t nfs fails with Stale file handle 的解决方法"></a><code>mount -t nfs</code> fails with <code>Stale file handle</code> 的解决方法</h2><p>最近，个人负责维护的服务器意外断电了，重新上电后NFS服务不能正常工作 —— NFS client不能挂载原先export的文件目录。尝试手动<code>mount -t nfs xxx</code>出现<code>mount.nfs: Stale file handle</code>的错误提示。最终通过<a href="https://unix.stackexchange.com/users/1131/maxschlepzig">maxschlepzig</a>提供的解决方案将问题处理了。</p><p>以下是原答案：</p><blockquote><p>A <code>mount -t nfs</code> fails with <code>Stale file handle</code> if the server has some stale exports entries for that client.</p><p>Example scenario: this might happen when the server reboots without the client umounting the nfs volumes first. When the server is back and the client then umounts and tries to mount the nfs volume the server might respond with:</p><p><code>mount.nfs: Stale file handle</code></p><p>You can check for this via looking at <code>/proc/fs/nfs/exports</code> or <code>/proc/fs/nfsd/exports</code>. If there is entry for the client it might be a stale one.</p><p>You can fix this via explicitly un-exporting and re-exporting the relevant exports on the server. For example to do this with all exports:</p><pre><code class="shell">exportfs -uacat /proc/fs/nfs/exportsexportfs -a</code></pre><p>After this the client’s <code>mount -t nfs ...</code> should succeed.</p><p>Note that mount yielding ESTALE is quite different from some other system call (like open/readdir/unlink/chdir …) returning ESTALE. It’s export being stale vs. a file handle being stale. A stale file handle easily happens with NFS (e.g. a client has a file handle but the file got deleted on the server).</p></blockquote><p>根据博主的描述，我确实在<code>/proc/fs/nfs/exports</code>中找到无效的共享路径。主要经过以下几步：</p><ol><li>在NFS服务所在机器上通过<code>exportfs -v</code>或<code>showmount -e</code>查看当前机器共享出来的路径；</li><li>查看<code>/proc/fs/nfs/exports</code>的共享路径是否和步骤1一致；</li><li>如果不一致，将<code>/proc/fs/nfs/exports</code>文件删除，并通过命令<code>exportfs -ua</code>取消当前共享；然后运行命令<code>exportfs -ra</code>重新共享路径；</li><li>检查<code>/proc/fs/nfs/exports</code>路径是否符合预期；</li><li>前往客户机执行<code>mount -t nfs ...</code>、<code>mount -a</code>等挂载命令进行NFS挂载。</li></ol><h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><ol><li><a href="https://unix.stackexchange.com/a/447581">https://unix.stackexchange.com/a/447581</a></li></ol>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h2 id=&quot;mount-t-nfs-fails-with-Stale-file-handle-的解决方法&quot;&gt;&lt;a href=&quot;#mount-t-nfs-fails-with-Stale-file-handle-的解决方法&quot; class=&quot;headerlink&quot;</summary>
        
      
    
    
    
    <category term="Bugfix" scheme="https://ukeyboard.github.io/categories/Bugfix/"/>
    
    
    <category term="Linux" scheme="https://ukeyboard.github.io/tags/Linux/"/>
    
    <category term="NFS" scheme="https://ukeyboard.github.io/tags/NFS/"/>
    
    <category term="File System" scheme="https://ukeyboard.github.io/tags/File-System/"/>
    
  </entry>
  
  <entry>
    <title>Representation Learning on Graphs</title>
    <link href="https://ukeyboard.github.io/2019/07/09/Representation-Learning-on-Graphs/"/>
    <id>https://ukeyboard.github.io/2019/07/09/Representation-Learning-on-Graphs/</id>
    <published>2019-07-09T13:41:25.000Z</published>
    <updated>2019-07-11T04:25:59.072Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Representation-Learning-on-Graphs-Methods-and-Applications"><a href="#Representation-Learning-on-Graphs-Methods-and-Applications" class="headerlink" title="Representation Learning on Graphs: Methods and Applications"></a>Representation Learning on Graphs: Methods and Applications</h2><blockquote><p>William L. Hamilton, Rex Ying, Jure Leskovec</p><p><a href="https://arxiv.org/pdf/1709.05584.pdf">https://arxiv.org/pdf/1709.05584.pdf</a></p><p>Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is ﬁnding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-deﬁned heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a uniﬁed framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.</p></blockquote><p>The central problem in machine learning on graphs is finding a way to incorporate information about graph structure into a ML model. The challenge - from a ML perspective - is that there is no straightforward way to encode this high-dimensional non-Euclidean information about graph structure into a feature vector.</p><p><strong>Extract structural information</strong>:  1) <em>Traditional methods</em> : summary graph statistics (e.g. degrees or clustering coefficients), kernel functions or carefully engineered features to measure local neighborhood structure.   2) <em>Learn to encode</em>: a surge of approaches that seek to learn representations that encode structural information about graph.</p><p><strong>Representation Learning</strong>: learn a mapping that embeds nodes or entire (sub)graphs, as points in a low-dimensional vector space $\mathbf{R}^d$. The goal is to optimize then mapping so that geometric relationships in the embedding space reflect the structure of original graph.</p><p><strong>Unsupervised Approaches</strong>: optimize the mapping only making use of information in $\mathbf{A}$ and $\mathbf{X}$ , without knowledge of a particular downstream ML task.</p><p><strong>Supervised Approaches</strong>: optimize the mapping, making use of classification or regression labels (for individual nodes or entire subgraphs).</p><p><strong>Notation</strong>:</p><table><thead><tr><th>Symbol</th><th>Desc</th></tr></thead><tbody><tr><td>$\mathcal{G}=(\mathcal{V},\mathcal{E})$</td><td>undirected graph</td></tr><tr><td>$\mathbf{A}$</td><td>Binary adjacency matrix</td></tr><tr><td>$\mathbf{X}\in\mathcal{R}^{m\times\vert\mathcal{V}\vert}$</td><td>Real-valued matrix of node attributes<br />e.g. representing text or metadata associated with nodes</td></tr><tr><td>$\mathbf{z}\in\mathcal{R}^d$</td><td>node or (sub)graph embedding, where $d \ll \vert\mathcal{V}\vert$</td></tr><tr><td>$\text{ENC:} \mathcal{V} \rightarrow \mathcal{R}^d$</td><td>encoder or the embedding function or mapping function</td></tr><tr><td>$v_i\in\mathcal{V}$</td><td>the $i$-th node in graph</td></tr><tr><td>$\mathbf{z}_i \in \mathcal{R}^d$</td><td>the embedding of the $i$-the node</td></tr><tr><td>DEC: $\mathcal{R}^d \times \mathcal{R}^d \rightarrow \mathcal{R}^{+}$</td><td>decoder</td></tr><tr><td>$s_{\mathcal{G}}$</td><td>the user-defined, graph-based similarity measure between nodes, defined over the graph $\mathcal{G}$</td></tr></tbody></table><h3 id="Embedding-Nodes"><a href="#Embedding-Nodes" class="headerlink" title="Embedding Nodes"></a>Embedding Nodes</h3><blockquote><p>Encode nodes as low-dimensional vectors that summarize their graph position and the structure of their local graph neighborhood, where geometric relations in this latent space correspond to interactions in the original graph.</p></blockquote><h4 id="AED-encoder-decoder"><a href="#AED-encoder-decoder" class="headerlink" title="AED: encoder-decoder"></a>AED: encoder-decoder</h4><p><strong>Encoder</strong>: map nodes to vector embeddings </p><p><strong>Decoder</strong>: accept a set of node embeddings and decodes user-specific graph statistics from the embeddings, for example: existence of edges, community the node belongs to. Most of the works use a basic pairwise decoder that maps pairs of node embeddings to a real-valued node similarity measure, which quantifies the similarity of the two nodes in the original graph.</p><p><strong>Objective</strong>: $\text{DEC}(\text{ENC}(v_i),\text{ENC}(v_j)) = \text{DEC}(\mathbf{z}_i,\mathbf{z}<em>j) \approx s</em>{\mathcal{G}}(v_i,v_j)$ </p><p>Most approaches defines a user-specified loss function, which measure the discrepancy between the decodeed similarity values $\text{DEC}(\mathbf{z}<em>i,\mathbf{z}<em>j)$ and the true values $s</em>{\mathcal{G}}(v_i,v_j)$ .<br>$$<br>\mathcal{L} = \sum</em>{(v_i,v_j)\in\mathcal{D}} \mathcal{l}(\text{DEC}(\mathbf{z}_i,\mathbf{z}<em>j), s</em>{\mathcal{G}}(v_i,v_j))<br>$$<br><strong>4 Methodological Components</strong>: </p><ol><li><strong>pairwise similarity function</strong>: $s_{\mathcal{G}}: \mathcal{V} \times \mathcal{V} \rightarrow \mathcal{R}^{+}$ defined over the graph $\mathcal{G}$ , measuring the similarity between nodes</li><li><strong>encoder function</strong> defines how to map nodes in graph into embedding vectors.</li><li><strong>decoder function</strong> defines how to reconstruct the pairwise similarity values.</li><li><strong>loss function</strong> defines how to compare the reconstructions and the true values.</li></ol><p><span style="color:red">Note: we ignore the shallow embedding approaches (i.e. Matrix Factorization &amp; Random walk). Please refer to the original paper for more information.</span></p><h4 id="Neighborhood-autoencoder-methods"><a href="#Neighborhood-autoencoder-methods" class="headerlink" title="Neighborhood autoencoder methods"></a>Neighborhood autoencoder methods</h4><table><thead><tr><th>Method</th><th>encoder</th><th>decoder</th><th>loss function</th></tr></thead><tbody><tr><td><a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf">DNGR</a></td><td>$\mathbf{z}_i = \text{ENC}(v_i)=\text{ENC}(\mathbf{s}_i)$, the pointwise mutual information of 2 nodes co-occurring on random walks , similar to DeepWalk, $\mathbf{S}$ contains pairwise node similarities</td><td>$\text{DEC}(\mathbf{z_i}) \approx \mathbf{s}_i$</td><td>$\mathcal{L}=\sum_{v_i\in\mathcal{V}}{\Vert\text{DEC}(\mathbf{z}_i) - \mathbf{s}_i\Vert}_2^2$</td></tr><tr><td><a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">SDNE</a></td><td>$\mathbf{z}_i = \text{ENC}(v_i)=\text{ENC}(\mathbf{s}_i)$, $\mathbf{s}_i \triangleq \mathbf{A}_i,$ as $v_i$’s adjacency vector, $\mathbf{S}$ contains pairwise node similarities</td><td>$\text{DEC}(\mathbf{z_i}) \approx \mathbf{s}_i$</td><td>$\mathcal{L}=\sum_{v_i\in\mathcal{V}}{\Vert\text{DEC}(\mathbf{z}_i) - \mathbf{s}_i\Vert}_2^2$</td></tr></tbody></table><p>Input vector $\mathbf{s}_i$ contains information about $v_i$’s local graph graph neighborhood. The goal is to compress the node’s neighborhood informatin into a low-dimensional vector.</p><h4 id="Neighborhood-aggregation-amp-Conv-Encoders"><a href="#Neighborhood-aggregation-amp-Conv-Encoders" class="headerlink" title="Neighborhood aggregation &amp; Conv-Encoders"></a>Neighborhood aggregation &amp; Conv-Encoders</h4><p>“<em>By designing encoders that rely on a node’s local neighborhood (by aggregating information from its local neighborhood), but not necessarily the entire graph</em>.”  </p><p><img src="https://i.loli.net/2019/07/10/5d25fc817f6cc69866.png" alt="A deep-2 version of information aggregation"></p><blockquote><p>In cases where attribute data is not given, these methods can use simple graph statistics as attributes (e.g., node degrees) , or assign each node a one-hot indicator vector as an attribute. <strong>These methods<br>are often called <em>convolutional</em> because they represent a node as a function of its surrounding neighborhood, in a manner similar to the receptive field of a center-surround convolutional kernel in computer vision.</strong> </p></blockquote><p><img src="https://i.loli.net/2019/07/10/5d260145b0acc42260.png" alt="Neighborhood-aggregation encoder algorithm"></p><p>The trainable parameters is a set of aggregation functions and a set of weight matrices ${\mathbf{W}^k, \forall k \in [1,K]}$ . Methods follow this algorithm often differ primarily in how the aggregation and vector combination are performed.</p><p><img src="https://i.loli.net/2019/07/11/5d26a3c5ca2fc22486.png" alt="Aggregation &amp; Combination"></p><p>The basic approach - average neighbor messages and apply a neurual network.<br>$$<br>\mathbf{h}_v^0 = \mathbf{x}_v \<br>\mathbf{h}_v^k = \sigma(\mathbf{W}<em>k\sum</em>{u\in N(v)}\frac{\mathbf{h}_u^{k-1}}{\vert N(v) \vert} + \mathbf{B}_k\mathbf{h}_v^{k-1}), \forall k\in[1,K] \<br>\mathbf{z}_v = \mathbf{h}_v^{K}<br>$$<br>Note: <em>the same set of aggregation functions are shared for all nodes, so is the model parameters. And all learnable parameters can generalize to unseen nodes.</em></p><p><strong>GCN</strong> : a slight variation of the aggregation functions.<br>$$<br>\mathbf{h}_v^k = \sigma(\mathbf{W}<em>k\sum</em>{u\in N(v)\cup v}\frac{\mathbf{h}_u^{k-1}}{\sqrt{\vert N(u) \vert\vert N(v) \vert}}), \forall k\in[1,K]<br>$$<br>GCN uses the same transformation matrix $\mathbf{W}_k$ for self and neighbor nodes, and normalizes embeddings adaptively across neighbors, instead of simple average.</p><p><strong>GraphSAGE</strong>: a general aggregation method rather than (weighted) average, and concatenating self embedding and neighbor enbedding .<br>$$<br>\mathbf{h}_v^k = \sigma([\mathbf{W}_k \cdot \text{AGG}({\mathbf{h}_u^{k-1},\forall u\in N(v)}),\mathbf{B}_k\mathbf{h}_v^{k-1}]), \forall k\in[1,K]<br>$$</p><p>we are free to employ proper aggregation method:<br>$$<br>\begin{align}<br>\text{AGG} = \sum_{u\in N(v)}\frac{\mathbf{h}_u^{k-1}}{\vert N(v)\vert} &amp; \quad (mean) \<br>\text{AGG} = \gamma({\mathbf{Q}\mathbf{h}_u^{k-1}, \forall u \in N(v)}) &amp; \quad (pool) \<br>\text{AGG} = \text{LSTM}([\mathbf{h}_u^{k-1}, \forall u \in \pi(N(v))]) &amp; \quad (LSTM)<br>\end{align}<br>$$<br>GCN &amp; GraphSAGE generally only 2-3 layers deep. To handle models with $\gt20$ layers, use LSTM or RNN as the shared network across layers. </p><p>**RNN &amp;&amp; LSTM ** :<br>$$<br>\mathbf{m}<em>v^k = \sum</em>{u\in N(v)} M(\mathbf{h}_u^{k-1},\mathbf{h}<em>v^{k-1},\mathbf{e}</em>{u,v}) \<br>\mathbf{h}_v^k = U(\mathbf{h}_v^{k-1},\mathbf{m}_v^k)<br>$$</p><h4 id="Graph-Neural-Networks"><a href="#Graph-Neural-Networks" class="headerlink" title="Graph Neural Networks"></a>Graph Neural Networks</h4><p>Instead of aggregating information from neighbors, the intuition behind GNNs is that graphs can be viewed as specifying scaffolding for a “message passing” algorithm between nodes.</p>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h2 id=&quot;Representation-Learning-on-Graphs-Methods-and-Applications&quot;&gt;&lt;a href=&quot;#Representation-Learning-on-Graphs-Methods-and-Applications&quot;</summary>
        
      
    
    
    
    <category term="Reading" scheme="https://ukeyboard.github.io/categories/Reading/"/>
    
    
    <category term="Graph Neural Network" scheme="https://ukeyboard.github.io/tags/Graph-Neural-Network/"/>
    
    <category term="Network Embedding" scheme="https://ukeyboard.github.io/tags/Network-Embedding/"/>
    
  </entry>
  
  <entry>
    <title>如何多页A4纸打印一张滚动长图</title>
    <link href="https://ukeyboard.github.io/2019/06/17/how-to-print-fullsize-screenshot-on-multi-a4-paper/"/>
    <id>https://ukeyboard.github.io/2019/06/17/how-to-print-fullsize-screenshot-on-multi-a4-paper/</id>
    <published>2019-06-17T01:14:39.000Z</published>
    <updated>2019-06-17T02:40:26.748Z</updated>
    
    <content type="html"><![CDATA[<h3 id="如何多页A4打印一张滚动长图"><a href="#如何多页A4打印一张滚动长图" class="headerlink" title="如何多页A4打印一张滚动长图"></a>如何多页A4打印一张滚动长图</h3><p>图片打印功能多是一张图一页纸，默认不支持跨页打印。</p><p>对于一张长图而言，默认将图片进行缩放使得一张A4能够容纳这张长图，必然导致打印内容过小看不清楚。如果选择适应页面宽度则图片仅部分可视，超出页面部分将不被打印。</p><p>针对这个问题网上有很多使用画图工具编辑、设置打印参数等的操作，今天给大家介绍一个简单的方法：</p><ul><li>直接使用Chrome 浏览器打开长图（可能需要点击🔍将图片网页全屏显示）</li><li>直接在Chrome里调出打印预览框，这时你会发现长图已经自动多页化处理</li><li>微调缩放比例，避免内容跨页显示</li><li>设置打印机参数（如双面打印等）点击打印即可</li></ul><h4 id="Safari-和-Chrome-预览和打印长图对比"><a href="#Safari-和-Chrome-预览和打印长图对比" class="headerlink" title="Safari 和 Chrome 预览和打印长图对比"></a>Safari 和 Chrome 预览和打印长图对比</h4><p><img src="https://i.loli.net/2019/06/17/5d06fc117c51216778.jpg" alt="Safari浏览器打印长图仅能单页"></p><p><img src="https://i.loli.net/2019/06/17/5d06fc127e32f63018.jpg" alt="Chrome浏览器长图打印可自动多页"></p><p>另外能发现Chrome浏览器除能够自动实现长图多页打印外，还支持将自动分页后的长图保存为PDF文件，这点是Safari 浏览器不支持的。通过这种方式我们可以将已保存的长图保存为多页PDF，曲线实现网页保存为PDF的功能，规避了原网页打印和显示样式不一致时无法直接保存为PDF的窘迫。</p>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h3 id=&quot;如何多页A4打印一张滚动长图&quot;&gt;&lt;a href=&quot;#如何多页A4打印一张滚动长图&quot; class=&quot;headerlink&quot;</summary>
        
      
    
    
    
    <category term="wiki" scheme="https://ukeyboard.github.io/categories/wiki/"/>
    
    
    <category term="Chrome" scheme="https://ukeyboard.github.io/tags/Chrome/"/>
    
    <category term="Screenshot" scheme="https://ukeyboard.github.io/tags/Screenshot/"/>
    
    <category term="Print" scheme="https://ukeyboard.github.io/tags/Print/"/>
    
  </entry>
  
  <entry>
    <title>如何将网页保存为滚动长图</title>
    <link href="https://ukeyboard.github.io/2019/06/17/how-to-save-web-page-as-fullsize-screenshot/"/>
    <id>https://ukeyboard.github.io/2019/06/17/how-to-save-web-page-as-fullsize-screenshot/</id>
    <published>2019-06-17T01:13:20.000Z</published>
    <updated>2019-06-17T02:41:51.121Z</updated>
    
    <content type="html"><![CDATA[<h3 id="如何将网页保存为滚动长图"><a href="#如何将网页保存为滚动长图" class="headerlink" title="如何将网页保存为滚动长图"></a>如何将网页保存为滚动长图</h3><p>日常工作中我们可能会有保存网页的需求。保存一个网页内容通常有三种途径：</p><ol><li>直接将网页保存为HTML格式，通常操作是在页面右键 —&gt; 选择”另存为” 或者直接快捷键 <code>Command+s</code> 触发另存为操作。网页内容将会保存在一个文件夹内，包括页面HTML源码和其他负责渲染和交互的CSS和Javascript文件。<img src="https://i.loli.net/2019/06/17/5d06eeddcfc2057925.jpg" alt="将网页内容保存到本地"> 这种保存方式的问题是不便于分享，打印效果依赖于原网页实现的打印样式（很多页面的打印效果和显示效果差别很大）。</li><li>将网页保存为PDF。这种方式比较适用于打印样式和显示效果相近的页面，我们可以直接 <code>Command+p</code> 调出页面打印功能，可在旁边预览框中观察实际打印效果，选择保存为PDF文件即可。</li><li>将网页保存为图片。这种方式应该是现阶段最好的保存方式了，原因有二：其一，保存的图片便于在社交平台上进行分享；其二，保存的图片与网页显示效果一致，不存在偏差。但也有个问题，如果网页内容很长，需要将网页保存为长图，一般的截图软件很难实现这个功能。</li></ol><p>本节将和大家分享如何将网页保存为长图的方法：</p><blockquote><p>需求：</p><p>系统不限；</p><p>Chrome 浏览器</p></blockquote><p>本节将于大家分享的方法依赖于Chrome浏览器，以保存<a href="https://tmuxcheatsheet.com/">https://tmuxcheatsheet.com/</a> 为长图示例。</p><ul><li>首先，使用Chrome浏览器打开网站，然后在网页任意位置右键，在弹出框选择 “Inspect” 调出开发者环境。用户可以通过其他方式打开开发者环境。</li><li>在开发者环境中选择左上角的 “Toggle device toolbar” </li><li>在页面顶部选择使用 “iPad Pro” 预览页面</li><li>点击右上角的三点线图标在下拉列表里选择 “Capture fullsize screenshot” 保存为滚动长图即可。</li></ul><p><img src="https://i.loli.net/2019/06/17/5d06f59d75aed22258.jpg" alt="Chrome实现网页保存为滚动长图"></p><p>通过上面的方法我们就能实现将网页保存为滚动长图的功能，不需要借助任何浏览器插件。另外，在保存页面滚动长图之前，我们还可以借助浏览器开发者工具对网页展示内容进行编辑，删除广告或着不需要的部分、调整特定部分的显示样式等，待到编辑完成后再执行上图第4步即可。</p>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h3 id=&quot;如何将网页保存为滚动长图&quot;&gt;&lt;a href=&quot;#如何将网页保存为滚动长图&quot; class=&quot;headerlink&quot;</summary>
        
      
    
    
    
    <category term="wiki" scheme="https://ukeyboard.github.io/categories/wiki/"/>
    
    
    <category term="Chrome" scheme="https://ukeyboard.github.io/tags/Chrome/"/>
    
    <category term="Screenshot" scheme="https://ukeyboard.github.io/tags/Screenshot/"/>
    
  </entry>
  
  <entry>
    <title>Matching Networks for One Shot Learning</title>
    <link href="https://ukeyboard.github.io/2019/06/16/Matching-Networks-for-One-Shot-Learning/"/>
    <id>https://ukeyboard.github.io/2019/06/16/Matching-Networks-for-One-Shot-Learning/</id>
    <published>2019-06-16T14:49:27.000Z</published>
    <updated>2019-06-16T15:00:05.796Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Matching-Networks-for-One-Shot-Learning"><a href="#Matching-Networks-for-One-Shot-Learning" class="headerlink" title="Matching Networks for One Shot Learning"></a>Matching Networks for One Shot Learning</h3><blockquote><p>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra</p><p><a href="https://arxiv.org/abs/1606.04080">https://arxiv.org/abs/1606.04080</a></p></blockquote><p>“<em>Many groups have investigated ways to augment NN architectures with external memories and other components. In these models, often a fully differentiable neural attention mechanism is defined to access a memory matrix which stores usefule information to solve the task at hand</em>“</p><p><strong>Train &amp; test condition must match</strong>: we train the network by showing only a few examples per class, switching the task from minibatch to minibatch, muche like how it will be tested when presented with a few examples of a new task.</p><h4 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h4><p><img src="https://i.loli.net/2019/06/16/5d065792df9de77166.png" alt="Matching Networks architecture"></p><p>We wish to map from a small support set of $k$ examples of image-label pairs $S=_{i=1}^k$ to a classifier $c_S$ which,  given a test example $\hat{x}$ , defines the probability distribution over outputs $\hat{y}$ . Our model computes $\hat{y}$ as follows:<br>$$<br>\displaystyle </p><p>\hat{y} = \sum_{i=1}^k \alpha(\hat{x},x_i)y_i<br>$$<br>where $(x_i,y_i)$ are samples in the support set $S$ , and $\alpha$ is an attention mechanism, e.g. softmax over the cosine distance over embedding feature.</p><p><strong>Full Context Embedding</strong>: The above classification strategy is fully conditioned on the whole support set $S$ through $P(\cdot \vert \hat{x},S)$. Attention mechanism is applied in the embedding space $X$ . In additon to mapping one example $x_i$ into the embedding space independently of other examples in the support set, we propose to embed $x_i$ conditioned on support set $S$, i.e.  from $g(x_i)$ to $g(x_i, S)$ . The function $g(x_i,S)$ can modify how to embed $x_i$ .  We use bi-LSTM to encode $x_i$ in the context of the support $S$ .</p><p><strong>Training</strong>:<br>$$<br>\displaystyle<br>\theta = \arg {\max}<em>{\theta} E</em>{L \sim T} [ E_{S \sim L, B \sim L} [{\sum}<em>{(x,y) \in B} \log P</em>{\theta}(y \vert x, S)] ]<br>$$<br>Define task $T$ as distribution over possible label set $L$ . A lable set $L$ is sampled from task $T$. The use $L$ to sample a support set $S$ and the test set $B$ . For instance, given $L={\text{cats,dogs}}$ , $S$ and $B$ are labelled examples of cats and dogs. </p><p>The matching network is trained over $(S,B)$ to minimize the error predicting the labels in the test set $B$ conditioned on the support set $S$. </p><h4 id="Experiment-Setup"><a href="#Experiment-Setup" class="headerlink" title="Experiment Setup"></a>Experiment Setup</h4><p><strong>N-way k-shot</strong>: a set of $k$ labelled examples from each of $N$ classes that have not previously trained upon.</p><p><strong>Omniglot</strong>: 1623 charactors from 50 different alphabets, 20 examples from different person / character .</p><p>A stack (specificly 4) of modules ( $3 \times 3 \times 64$ conv + bn + relu + $2 \times 2$ pooling) map one $28 \times 28$ image to a $1 \times 1 \times 64 $ embedding. A fully connected layer followed by a softmax non-linearity is used to define the Baseline Classifier.</p><p><strong>miniImageNet</strong>: $60,000$ color images of size $84 \times 84$ with 100 classes, each having 600 examples. More complex than CIFAR10, but fits in memory on modern machines. 80 classes for train and the remaining 20 classes for testing.</p><p><strong>randImageNet</strong>: Randomly select 118  classes $L_{\text{rand}}$  for testing, and all other classes with $L{\text{rand}}$ removed for train.</p><p><strong>dogsImageNet</strong>: Use classes descended from dogs $L_{\text{dogs}}$ for testing, and all other classes with $L_{\text{dogs}}$ removed for train.</p><p><img src="https://i.loli.net/2019/06/16/5d0622499addc38431.png" alt="Results on the Omniglot dataset"></p><p><img src="https://i.loli.net/2019/06/16/5d0622499345e82968.png" alt="Results on miniImageNet"></p><p><img src="https://i.loli.net/2019/06/16/5d0622498bac341702.png" alt="Results on full ImageNet on rand and dogs one-shot tasks"></p>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h3 id=&quot;Matching-Networks-for-One-Shot-Learning&quot;&gt;&lt;a href=&quot;#Matching-Networks-for-One-Shot-Learning&quot; class=&quot;headerlink&quot; title=&quot;Matching</summary>
        
      
    
    
    
    <category term="Reading" scheme="https://ukeyboard.github.io/categories/Reading/"/>
    
    
    <category term="Few-Shot Learning" scheme="https://ukeyboard.github.io/tags/Few-Shot-Learning/"/>
    
    <category term="Low-Shot Learning" scheme="https://ukeyboard.github.io/tags/Low-Shot-Learning/"/>
    
    <category term="One-Shot Learning" scheme="https://ukeyboard.github.io/tags/One-Shot-Learning/"/>
    
  </entry>
  
  <entry>
    <title>One-shot Learning with Memory-Augmented Neural Networks</title>
    <link href="https://ukeyboard.github.io/2019/06/16/One-shot-Learning-with-Memory-Augmented-Neural-Networks/"/>
    <id>https://ukeyboard.github.io/2019/06/16/One-shot-Learning-with-Memory-Augmented-Neural-Networks/</id>
    <published>2019-06-16T13:23:51.000Z</published>
    <updated>2019-06-16T14:37:50.431Z</updated>
    
    <content type="html"><![CDATA[<h3 id="One-shot-Learning-with-Memory-Augmented-Neural-Networks"><a href="#One-shot-Learning-with-Memory-Augmented-Neural-Networks" class="headerlink" title="One-shot Learning with Memory-Augmented Neural Networks"></a>One-shot Learning with Memory-Augmented Neural Networks</h3><blockquote><p>Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap</p><p><a href="https://arxiv.org/abs/1605.06065">https://arxiv.org/abs/1605.06065</a></p><p>Code: <a href="https://github.com/philippe554/MANN">https://github.com/philippe554/MANN</a></p></blockquote><p>“<em>Omniglog images $x_t$ are presented with time-offset labels $y_{t-1}$ to prevent the network from simply mapping the class labels to the output. Frjom episode to episode, the classes to be presented in the episode, their associated labels, and the specific samples are all shuffled.</em>“</p><p><img src="https://i.loli.net/2019/06/14/5d02fe0c529ae80618.jpg" alt="Framework &amp; Strategory"></p><p>The controller employed in our model are either LSTMs or feed-forward networks. The controller interacts with an external memory module using read and write heads, which act to retrieve representations from memory or place them into memory, respectively. </p><h4 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h4><p>Given input image $x_t$ , the controller produces a key $k_t$, which is then either store into the external memory $M_t$ or used to retrieve a particular memory $M_t(i)$ from a row.</p><p><strong>Memory Retrieving</strong>:  $\displaystyle \mathbf{r_t} \leftarrow \sum_t{w_t^r(i)\mathbf{M_t(i)}}$   </p><p>where $\mathbf{w_t^r}$ is the read-weight vector with elements computed according to a softmax:<br>$$<br>w_t^r(i) \leftarrow \frac{\exp{(K(\mathbf{k_t} , \mathbf{M_t(i)}))}}{\sum_j{\exp{(K(\mathbf{k_t} , \mathbf{M_t(j)}))}}}<br>$$</p><p>where  $K(\mathbf{k}_t , \mathbf{M}_t(i)) = \frac{\mathbf{k}_t \cdot \mathbf{M}_t(i)}{\Vert \mathbf{k}_t \Vert\Vert \mathbf{M}_t(i) \Vert}$  denotes cosine distance.</p><p>The retrieved memory is used by the controller as the input to a classifier, such as a softmax output layer, and as an additional input for the next controller state.</p><p><strong>LRUA (Least Recently Used Access)</strong>: a pure content-based memory writer that writes memories to either the least used memory slot or the most recently used memory slot. New information is written into rarely-used locations, preserving recently encoded information, or it is written into the last used location, which can function as an update of the memory with newer, possibly more relevant information.</p><p>We keep an usage weight $\mathbf{w_t^u}$ which is updated at each time-step by decaying the previous usage weight (via decay parameter $\gamma$ ) and adding the current read &amp; write weight<br>$$<br>\mathbf{w_t^u = \gamma w_{t-1}^u + w_t^r + w_t^w}<br>$$<br>The least-used weight $\mathbf{w_t^{lu}}$ can be computed using $\mathbf{w_t^u}$<br>$$<br>w_t^{lu}(i) =<br>\begin{cases}<br>0 \quad \text{if } w_t^u(i) \gt m(\mathbf{w_t^u,n}) \<br>1 \quad \text{if } w_t^u(i) \leq m(\mathbf{w_t^u,n})<br>\end{cases}<br>$$</p><p>where $m(\mathbf{w_t^u},n)$ denotes the $n^{th}$ smallest element of vector $\mathbf{w_t^u}$ , $n$ is set to equal the number of reads to memory. </p><p><strong>Memory Write</strong>:  we obtain the write weights<br>$$<br>\mathbf{w_t^w} \leftarrow \sigma(\alpha) \mathbf{w_{t-1}^r} + (1-\sigma(\alpha))\mathbf{w_{t-1}^{lu}}<br>$$</p><p>Here $\sigma(\cdot)$ is softmax function, $\alpha$ is a learnable gate parameter to interpolate between the weights.</p><p>Before memory writing, the least used memory slot  is set to zero. Then write the memory<br>$$<br>\mathbf{M}<em>t(i) \leftarrow \mathbf{M}</em>{t-1}(i) + w_t^w(i)\mathbf{k}_t, \forall i<br>$$</p><p>Thus, memories can be written into the zeroed memory slot or the previously used slot. If it is the later case, the least used memories simply get erased.</p><h4 id="Experiment-Setup"><a href="#Experiment-Setup" class="headerlink" title="Experiment Setup"></a>Experiment Setup</h4><table><thead><tr><th>Dataset</th><th>Aug</th><th>Others</th></tr></thead><tbody><tr><td>Omniglot</td><td>Randomly translating and rotating</td><td>create new classes through ${90}^{\circ}, {180}^{\circ}, {270}^{\circ}$ rotations</td></tr></tbody></table><p>A good strategy to employ in this classification task is to wipe the external memory from episode to episode. Since each episode contians examples of different classes with unique labels, any information persisting in memory across episodes inevitably acts as interference for the episode at hand. In order to test the memory interference, we performed the classification task without wiping the memory across episodes. </p><table><thead><tr><th><img src="https://i.loli.net/2019/06/14/5d0376c482cdd11049.png" alt="Performance of MANN model"></th><th><img src="https://i.loli.net/2019/06/14/5d0376c47e1ec11205.png" alt="Performance of LSTM model"></th></tr></thead><tbody><tr><td><img src="https://i.loli.net/2019/06/14/5d0376c482cdd11049.png" alt="Performance of MANN model"></td><td><img src="https://i.loli.net/2019/06/14/5d0376c48078423239.png" alt="Performance of MANN model without memory wiping"></td></tr></tbody></table><p>Persisting memory across episodes makes the network less robust in its ability to achieve high accuracy. In our experiment, learning progressed much slower than that in the memory wiped case, and the model did not produce the characteristic fast spike in accuracy as seen in the memory-wiped condition. But interestingly, the network was able to perform comparably under certain setups.</p>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h3 id=&quot;One-shot-Learning-with-Memory-Augmented-Neural-Networks&quot;&gt;&lt;a href=&quot;#One-shot-Learning-with-Memory-Augmented-Neural-Networks&quot;</summary>
        
      
    
    
    
    <category term="Reading" scheme="https://ukeyboard.github.io/categories/Reading/"/>
    
    
    <category term="Few-Shot Learning" scheme="https://ukeyboard.github.io/tags/Few-Shot-Learning/"/>
    
    <category term="Low-Shot Learning" scheme="https://ukeyboard.github.io/tags/Low-Shot-Learning/"/>
    
    <category term="MANN" scheme="https://ukeyboard.github.io/tags/MANN/"/>
    
  </entry>
  
  <entry>
    <title>Synthesized classifier for zero-shot learning</title>
    <link href="https://ukeyboard.github.io/2019/06/16/synthesized-classifier-for-zero-shot-learning/"/>
    <id>https://ukeyboard.github.io/2019/06/16/synthesized-classifier-for-zero-shot-learning/</id>
    <published>2019-06-16T11:21:42.000Z</published>
    <updated>2019-06-16T14:48:12.401Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Synthesized-Classifiers-for-Zero-Shot-Learning"><a href="#Synthesized-Classifiers-for-Zero-Shot-Learning" class="headerlink" title="Synthesized Classifiers for Zero-Shot Learning"></a>Synthesized Classifiers for Zero-Shot Learning</h3><blockquote><p>Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, Fei Sha</p><p><a href="https://arxiv.org/abs/1603.00550">https://arxiv.org/abs/1603.00550</a></p></blockquote><p>“<em>We introduce a set of phantom object classes which do not correspond to and are not optimized to recognize any real classes directly. Any real class is a convex combinations of the coordinates of those plantom classes. When we need to construct a classifier for an unseen class, we will compute the convex combination coefficients from this class’s semantic space coordinates and use them to form the corresponding classifier.</em>“</p><p><strong>Two key interwoven challenges</strong>: </p><ul><li>How to relate unsean classes to seen ones</li><li>How to attain optimal discriminative performance on the unseen classes even though we do not have their labeled data</li></ul><p><img src="https://i.loli.net/2019/06/13/5d01f7f8da84a15077.jpg"></p><p>Tackle the problem from the perspective of manifold learning — align the semantic space and the model space. </p><blockquote><p>Each class $c$ has a coordinate  $a_c$ in the semantic embedding space. Two types of such spaces are explored: attributes &amp; class name embedding via word vectors. In the paper, we use attributes to illustrate the idea and in the experiments we test the approach on both types.</p></blockquote><p><strong>Semantic Space</strong>: semantic descriptions of object classes — attributes or class name — that do not directly examine visual appearances at the lowest level.</p><p>A weighted graph where the nodes correspond to object class names and the weights of the edges represent how they are related.</p><p><strong>Model Space</strong>: classifier for recognizing visual images of those classes that concerns itself largely for recognizing low level features.</p><p>The parameters for each object model are nothing but coordinates in the model space.</p><h4 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h4><p><strong>Notations</strong>:</p><p>$D={(x_n \in R^D, y_n)}_n^N$ — Training images  from seen classes $S={1,2,…,S }$.</p><p>$U={S+1,S+2,…,S+U}$ —  unseen classes. </p><p>A classifier assigns $\hat{y}$ to a data point $x$ is defined by $w_c \in R^D$:<br>$$<br>\hat{y} = \arg \max\limits_c  w_c^Tx<br>$$<br>Given $R$ virtual or plantom classes, the real classes and the plantom classes form a bipartite graph, the weights defined as<br>$$<br>s_{cr} = \frac{\exp{-d(a_c, b_r)}}{\sum_{r=1}^R \exp {-d(a_c,b_r)}}<br>$$<br>$s_{cr}$ can be interpreted as the conditional probability of observing class $r$ in the neighborhood of class $c$.  </p><p><strong>Learning Semantic Embedding</strong>  $b_r$:<br>$$<br>\displaystyle<br>b_r = \sum_{c=1}^S \beta_{rc}\alpha_c, \forall r \in {1,2,…,R}<br>$$<br><strong>Learning Virtual Classifier</strong>  $v_r$:<br>$$<br>\displaystyle<br>\min_{v_1,…,v_R} \sum_{c=1}^S \sum_{n=1}^N l(x_n,\parallel_{y_n,c};w_c) + \frac{\lambda}{2}\sum_{c=1}^S{\Vert w_c \Vert}<em>2^2<br>$$<br>s.t.  $w_c = \sum</em>{r=1}^R s_{cr}v_r, \forall c \in \Tau = {1,2,…,S}$.  </p><p>We perform alternating optimization for minimizing the following objective function:<br>$$<br>\displaystyle<br>\min_{ {v_r}<em>{r=1}^R,{\beta</em>{rc}}<em>{r,c=1}^{R,S}} \sum</em>{c=1}^S\sum_{n=1}^N l(x_n,\parallel_{y_n,c};w_c) + \frac{\lambda}{2}\sum_{c=1}^S{\Vert w_c\Vert}<em>2^2 + \eta\sum</em>{r,c=1}^{R,S} \vert \beta_{rc} \vert + \frac{\lambda}{2} \sum_{r=1}^R ({\Vert b_r \Vert}<em>2^2 - h^2)^2<br>$$<br>s.t. $w_c = \sum</em>{r=1}^R s_{cr}v_r, \forall c \in \Tau = {1,2,…,S}$.  </p><h4 id="Experiment-Setup"><a href="#Experiment-Setup" class="headerlink" title="Experiment Setup"></a>Experiment Setup</h4><p><strong>Semantic Space</strong>: </p><table><thead><tr><th>Dataset</th><th>Attr</th><th>Cls Name</th></tr></thead><tbody><tr><td>AwA</td><td>85-d binary or continuous attributes</td><td>100-d word vector derived from class name<br />1k-d word vectors extracted by Fu et al.</td></tr><tr><td>CUB</td><td>312-d continuous attributes<br />&amp; thresh to obtain binary attributes</td><td>NA</td></tr><tr><td>SUN</td><td>102-d continuous attributes<br />&amp; thresh to obtain binary attributes</td><td>NA</td></tr><tr><td>ImageNet</td><td>Not clear</td><td>500-d word vector from Skip-gram model on Wikipedia</td></tr></tbody></table><p><strong>Model Space</strong>: </p><p>Whenever possible, use shallow visual features such as color histogram, SIFT, PHOG, Fisher</p><p>For deep learning feature:</p><table><thead><tr><th>Dataset</th><th>Feat</th></tr></thead><tbody><tr><td>AwA</td><td>AlexNet &amp; GoogleNet</td></tr><tr><td>CUB</td><td>AlexNet &amp; GoogleNet</td></tr><tr><td>SUN</td><td>GoogleNet</td></tr><tr><td>ImageNet</td><td>GoogleNet</td></tr></tbody></table><p>For AlexNet, use the 4096-d activations of the penultimate layer (fc7) as feature.</p><p>For GoogleNet, use the 1024-d activations of the pooling units.</p>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h3 id=&quot;Synthesized-Classifiers-for-Zero-Shot-Learning&quot;&gt;&lt;a href=&quot;#Synthesized-Classifiers-for-Zero-Shot-Learning&quot; class=&quot;headerlink&quot;</summary>
        
      
    
    
    
    <category term="Reading" scheme="https://ukeyboard.github.io/categories/Reading/"/>
    
    
    <category term="Recognization" scheme="https://ukeyboard.github.io/tags/Recognization/"/>
    
    <category term="Zero-Shot Learning" scheme="https://ukeyboard.github.io/tags/Zero-Shot-Learning/"/>
    
  </entry>
  
  <entry>
    <title>关于Matlab 2012b License过期的问题</title>
    <link href="https://ukeyboard.github.io/2019/06/15/matlab-2012b-license-expired-issuse/"/>
    <id>https://ukeyboard.github.io/2019/06/15/matlab-2012b-license-expired-issuse/</id>
    <published>2019-06-15T09:52:08.000Z</published>
    <updated>2019-06-15T10:42:28.387Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于Matlab-2012b-License过期的问题"><a href="#关于Matlab-2012b-License过期的问题" class="headerlink" title="关于Matlab 2012b License过期的问题"></a>关于Matlab 2012b License过期的问题</h1><p>Matlab 2012b 破解文件提供的密钥文件原本是可用、有效的密钥文件，早期安装后能够正常使用matlab，但近期出现打开matlab提示需要重新激活的问题。Google搜索显示这是因为license文件过期导致的。用户可用文本编辑器打开 <code>C:\Program Files\MATLAB\R2012b\licenses</code> 下的license文件查看，会发现license的有效期只到2017年11月11号。</p><p><img src="https://i.loli.net/2019/06/15/5d04c0264d17d24606.png" alt="有效期至2017年11月11日"></p><p>用户可替换license文件中所有的 <code>MLM 28 11-nov-2017</code> 为 <code>MLM 99 permanent uncounted</code> 然后重新打开matlab 2012b看看是否有效。经本地实测，直接替换修改无效。</p><p>用户可以选择使用下面新的license，实测有效：</p><pre><code class="bash">INCREMENT Aerospace_Blockset MLM 99 permanent uncounted \        A05070F00D1EB1F92326 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=216 SN=888888 TS_OKINCREMENT Aerospace_Toolbox MLM 99 permanent uncounted \        6090F0C08395D4289512 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=237 SN=888888 TS_OKINCREMENT Bioinformatics_Toolbox MLM 99 permanent uncounted \        40E0B0406DE56D23A426 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=190 SN=888888 TS_OKINCREMENT Cert_Kit_IEC MLM 99 permanent uncounted \        9080309041D2DCCB2B10 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=179 SN=888888 TS_OKINCREMENT Communication_Blocks MLM 99 permanent uncounted \        80E010304ACCEEB5E0AA VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=77 SN=888888 TS_OKINCREMENT Communication_Toolbox MLM 99 permanent uncounted \        C0003000770A1A086530 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=162 SN=888888 TS_OKINCREMENT Compiler MLM 99 permanent uncounted 60D0C0F0DC03C2F72FF7 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=232 SN=888888 TS_OKINCREMENT Control_Toolbox MLM 99 permanent uncounted \        6020E0B00B7ECE0893BB VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=7 \        SN=888888 TS_OKINCREMENT Curve_Fitting_Toolbox MLM 99 permanent uncounted \        10703080061FF9DA5A81 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=229 SN=888888 TS_OKINCREMENT Data_Acq_Toolbox MLM 99 permanent uncounted \        E0B0E0E05124CF4A1A8D VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=21 SN=888888 TS_OKINCREMENT Database_Toolbox MLM 99 permanent uncounted \        0010D0B02CA7353F8314 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=230 SN=888888 TS_OKINCREMENT Datafeed_Toolbox MLM 99 permanent uncounted \        40905060D1ECD939538F VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=185 SN=888888 TS_OKINCREMENT Dial_and_Gauge_Blocks MLM 99 permanent uncounted \        A0F05070AA9AEBF40588 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=226 SN=888888 TS_OKINCREMENT Distrib_Computing_Toolbox MLM 99 permanent uncounted \        5000D00031D2C6B89F9C VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=229 SN=888888 TS_OKINCREMENT EDA_Simulator_Link MLM 99 permanent uncounted \        E070C0C02B4B90B5A8D0 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=205 SN=888888 TS_OKINCREMENT Econometrics_Toolbox MLM 99 permanent uncounted \        505010F00EB1289A73BA VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=149 SN=888888 TS_OKINCREMENT Embedded_IDE_Link MLM 99 permanent uncounted \        90E01090A9BDA4FF67D7 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=254 SN=888888 TS_OKINCREMENT Excel_Link MLM 99 permanent uncounted E0800080FED6DF10E43F \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=207 SN=888888 TS_OKINCREMENT Filter_Design_HDL_Coder MLM 99 permanent uncounted \        3050B02039903259577E VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=181 SN=888888 TS_OKINCREMENT Filter_Design_Toolbox MLM 99 permanent uncounted \        3000E0B02C875CDF5823 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=239 SN=888888 TS_OKINCREMENT Fin_Derivatives_Toolbox MLM 99 permanent uncounted \        70A010D017A8FA7CE955 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=247 SN=888888 TS_OKINCREMENT Financial_Toolbox MLM 99 permanent uncounted \        20E050A0ACC6A69E7AEE VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=241 SN=888888 TS_OKINCREMENT Fixed-Point_Blocks MLM 99 permanent uncounted \        E0206040ED6EF9BCCB3B VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=11 SN=888888 TS_OKINCREMENT Fixed_Income_Toolbox MLM 99 permanent uncounted \        50E090D0C130F6416176 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=132 SN=888888 TS_OKINCREMENT Fixed_Point_Toolbox MLM 99 permanent uncounted \        C07000703494010EF55C VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=154 SN=888888 TS_OKINCREMENT Fuzzy_Toolbox MLM 99 permanent uncounted \        D04010B048FB8D350E4E VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=233 SN=888888 TS_OKINCREMENT GADS_Toolbox MLM 99 permanent uncounted \        50D0900087384B51643A VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=210 SN=888888 TS_OKINCREMENT Identification_Toolbox MLM 99 permanent uncounted \        00707080A0CC0D8FB71E VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=190 SN=888888 TS_OKINCREMENT Image_Acquisition_Toolbox MLM 99 permanent uncounted \        50D0D0905C22130C4D2A VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=208 SN=888888 TS_OKINCREMENT Image_Toolbox MLM 99 permanent uncounted \        B01030E0F517B496275C VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=133 SN=888888 TS_OKINCREMENT Instr_Control_Toolbox MLM 99 permanent uncounted \        80C0505033C162F3017F VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=169 SN=888888 TS_OKINCREMENT MAP_Toolbox MLM 99 permanent uncounted C090E0E022BD3BA852B3 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=4 SN=888888 TS_OKINCREMENT MATLAB_Report_Gen MLM 99 permanent uncounted \        80B000B00052F620FB11 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=190 SN=888888 TS_OKINCREMENT MATLAB_Distrib_Comp_Engine MLM 99 permanent uncounted \        804080300CB545FBECE8 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=233 SN=888888 TS_OKINCREMENT MATLAB_Builder_for_Java MLM 99 permanent uncounted \        404040707C2CA5639DEE VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=186 SN=888888 TS_OKINCREMENT MATLAB_Builder_for_dot_Net MLM 99 permanent uncounted \        8070E0D0FFAA645DA39D VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=255 SN=888888 TS_OKINCREMENT MATLAB MLM 99 permanent uncounted 50E0F0D08F6CEF24CC5B \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=14 SN=888888 TS_OKINCREMENT MATLAB_Excel_Builder MLM 99 permanent uncounted \        50106010BCBF479623D9 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=206 SN=888888 TS_OKINCREMENT MBC_Toolbox MLM 99 permanent uncounted B05000C02F157C5BEE28 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=198 SN=888888 TS_OKINCREMENT MPC_Toolbox MLM 99 permanent uncounted 3050206087BF1C33045E \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=203 SN=888888 TS_OKINCREMENT Neural_Network_Toolbox MLM 99 permanent uncounted \        D0009010451EEB5A283F VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=198 SN=888888 TS_OKINCREMENT OPC_Toolbox MLM 99 permanent uncounted 50F070404BE7E269740A \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=198 SN=888888 TS_OKINCREMENT Optimization_Toolbox MLM 99 permanent uncounted \        E0F00030546D043DF202 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=152 SN=888888 TS_OKINCREMENT PDE_Toolbox MLM 99 permanent uncounted 5080705081D393548E6E \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=166 SN=888888 TS_OKINCREMENT Power_System_Blocks MLM 99 permanent uncounted \        90405000B77AC9D2CBB9 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=216 SN=888888 TS_OKINCREMENT Qual_Kit_DO MLM 99 permanent uncounted D01010C04E05A80BCD10 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=190 SN=888888 TS_OKINCREMENT RF_Blockset MLM 99 permanent uncounted A0A00020B1FED5C15465 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=201 SN=888888 TS_OKINCREMENT RF_Toolbox MLM 99 permanent uncounted 90F0905068868B4A97C8 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=168 SN=888888 TS_OKINCREMENT RTW_Embedded_Coder MLM 99 permanent uncounted \        A0D0E0C0B3978D64A1A3 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=250 SN=888888 TS_OKINCREMENT Real-Time_Win_Target MLM 99 permanent uncounted \        300060D0E68A928A0DBC VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=208 SN=888888 TS_OKINCREMENT Real-Time_Workshop MLM 99 permanent uncounted \        7020D04006040066FAEA VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=134 SN=888888 TS_OKINCREMENT Robust_Toolbox MLM 99 permanent uncounted \        1040201098A2D1E80D25 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=173 SN=888888 TS_OKINCREMENT SIMULINK MLM 99 permanent uncounted 30D05030E2F9BBB8FCC2 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=213 SN=888888 TS_OKINCREMENT SIMULINK_Report_Gen MLM 99 permanent uncounted \        E070B070769E869B6A03 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=206 SN=888888 TS_OKINCREMENT SL_Verification_Validation MLM 99 permanent uncounted \        C0309000FB238CC772CC VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=251 SN=888888 TS_OKINCREMENT Signal_Blocks MLM 99 permanent uncounted \        F000A0702665C46E07FF VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=196 SN=888888 TS_OKINCREMENT Signal_Toolbox MLM 99 permanent uncounted \        E070B0403768324D14C1 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=185 SN=888888 TS_OKINCREMENT SimBiology MLM 99 permanent uncounted 404080202CFCE52296A3 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=187 SN=888888 TS_OKINCREMENT SimDriveline MLM 99 permanent uncounted \        B030A0704195F94612EF VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=163 SN=888888 TS_OKINCREMENT SimElectronics MLM 99 permanent uncounted \        F0D050A035E4883B1E9D VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=246 SN=888888 TS_OKINCREMENT SimEvents MLM 99 permanent uncounted 30E02040EB0C922C71A0 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=216 SN=888888 TS_OKINCREMENT SimHydraulics MLM 99 permanent uncounted \        C07090B0F394787D61B7 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=185 SN=888888 TS_OKINCREMENT SimMechanics MLM 99 permanent uncounted \        909000103231C13BB4BB VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=148 SN=888888 TS_OKINCREMENT Simscape MLM 99 permanent uncounted 8020D0906FAC13D095EC \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=211 SN=888888 TS_OKINCREMENT Simulink_Control_Design MLM 99 permanent uncounted \        D09060704E819BCDDD32 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=206 SN=888888 TS_OKINCREMENT Simulink_Design_Optim MLM 99 permanent uncounted \        6020C080F9B2D2F92E1C VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=243 SN=888888 TS_OKINCREMENT Simulink_Design_Verifier MLM 99 permanent uncounted \        8080D0002C390CBB546E VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=196 SN=888888 TS_OKINCREMENT Simulink_HDL_Coder MLM 99 permanent uncounted \        F05000C0108BB935AD39 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=215 SN=888888 TS_OKINCREMENT Simulink_PLC_Coder MLM 99 permanent uncounted \        803090F0C42068269D45 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=194 SN=888888 TS_OKINCREMENT Spline_Toolbox MLM 99 permanent uncounted \        30E0B0A014330227515B VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=112 SN=888888 TS_OKINCREMENT Stateflow_Coder MLM 99 permanent uncounted \        2050D0B06F85F87E216B VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=199 SN=888888 TS_OKINCREMENT Stateflow MLM 99 permanent uncounted 4030C0C01E0B917BED3F \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=223 SN=888888 TS_OKINCREMENT Statistics_Toolbox MLM 99 permanent uncounted \        7010C05033AC51015B7C VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=178 SN=888888 TS_OKINCREMENT Symbolic_Toolbox MLM 99 permanent uncounted \        80D09090A1ADD082F35D VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=33 SN=888888 TS_OKINCREMENT SystemTest MLM 99 permanent uncounted B0A0A0E039DA5A2F4AA0 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=29 SN=888888 TS_OKINCREMENT Target_Support_Package MLM 99 permanent uncounted \        207000609E1847CC5A6E VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=172 SN=888888 TS_OKINCREMENT Vehicle_Network_Toolbox MLM 99 permanent uncounted \        00C02010ABF2A8B8152B VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=11 SN=888888 TS_OKINCREMENT Video_and_Image_Blockset MLM 99 permanent uncounted \        107070B0AF7901361B1E VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=162 SN=888888 TS_OKINCREMENT Virtual_Reality_Toolbox MLM 99 permanent uncounted \        B010002042F6C767442D VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=205 SN=888888 TS_OKINCREMENT Wavelet_Toolbox MLM 99 permanent uncounted \        30C0F0800F9B41DD8577 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=238 SN=888888 TS_OKINCREMENT XPC_Embedded_Option MLM 99 permanent uncounted \        102000905C51C5DE4BF0 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=174 SN=888888 TS_OKINCREMENT XPC_Target MLM 99 permanent uncounted 6090B0000421BEBA6810 \        VENDOR_STRING=QQ=47399897 HOSTID=ANY ck=136 SN=888888 TS_OKINCREMENT Sensor_Array_Toolbox MLM 99 permanent uncounted \        E0008030D7E6251042F1 VENDOR_STRING=QQ=47399897 HOSTID=ANY \        ck=179 SN=888888 TS_OK</code></pre>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h1 id=&quot;关于Matlab-2012b-License过期的问题&quot;&gt;&lt;a href=&quot;#关于Matlab-2012b-License过期的问题&quot; class=&quot;headerlink&quot; title=&quot;关于Matlab 2012b</summary>
        
      
    
    
    
    <category term="Tools" scheme="https://ukeyboard.github.io/categories/Tools/"/>
    
    
    <category term="Matlab" scheme="https://ukeyboard.github.io/tags/Matlab/"/>
    
    <category term="Issuse" scheme="https://ukeyboard.github.io/tags/Issuse/"/>
    
  </entry>
  
  <entry>
    <title>hexo-theme-matery主题说明文档</title>
    <link href="https://ukeyboard.github.io/2019/06/14/how-to-use-hexo-theme-matery/"/>
    <id>https://ukeyboard.github.io/2019/06/14/how-to-use-hexo-theme-matery/</id>
    <published>2019-06-14T01:15:32.000Z</published>
    <updated>2019-06-14T00:45:28.997Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hexo-theme-matery-主题说明文档"><a href="#hexo-theme-matery-主题说明文档" class="headerlink" title="hexo-theme-matery 主题说明文档"></a>hexo-theme-matery 主题说明文档</h1><p><a href="README.md">English Document</a> | <a href="https://blinkfox.github.io/">演示示例</a> | QQ 交流群: <code>926552981</code></p><blockquote><p>这是一个采用 <code>Material Design</code> 和响应式设计的 Hexo 博客主题。</p></blockquote><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul><li>简单漂亮，文章内容美观易读</li><li><a href="https://material.io/">Material Design</a> 设计</li><li>响应式设计，博客在桌面端、平板、手机等设备上均能很好的展现</li><li>首页轮播文章及每天动态切换 <code>Banner</code> 图片</li><li>瀑布流式的博客文章列表（文章无特色图片时会有 <code>24</code> 张漂亮的图片代替）</li><li>时间轴式的归档页</li><li><strong>词云</strong>的标签页和<strong>雷达图</strong>的分类页</li><li>丰富的关于我页面（包括关于我、文章统计图、我的项目、我的技能、相册等）</li><li>可自定义的数据的友情链接页面</li><li>支持文章置顶和文章打赏</li><li>支持 <code>MathJax</code></li><li><code>TOC</code> 目录</li><li>可设置复制文章内容时追加版权信息</li><li>可设置阅读文章时做密码验证</li><li><a href="https://gitalk.github.io/">Gitalk</a>、<a href="https://imsun.github.io/gitment/">Gitment</a>、<a href="https://valine.js.org/">Valine</a> 和 <a href="https://disqus.com/">Disqus</a> 评论模块（推荐使用 <code>Gitalk</code>）</li><li>集成了<a href="http://busuanzi.ibruce.info/">不蒜子统计</a>、谷歌分析（<code>Google Analytics</code>）和文章字数统计等功能</li><li>支持在首页的音乐播放和视频播放功能</li></ul><h2 id="贡献者"><a href="#贡献者" class="headerlink" title="贡献者"></a>贡献者</h2><p>感谢下面列出的贡献者，没有他们，hexo-theme-matery 不会这么完美。</p><ul><li><a href="https://github.com/HarborZeng">@HarborZeng</a></li></ul><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>当你看到这里的时候，应该已经有一个自己的 <a href="https://hexo.io/zh-cn/">Hexo</a> 博客了。如果还没有的话，不妨使用 Hexo 和 <a href="https://www.appinn.com/markdown/">Markdown</a> 来写博客和文章。</p><p>点击 <a href="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master">这里</a> 下载 <code>master</code> 分支的最新稳定版的代码，解压缩后，将 <code>hexo-theme-matery</code> 的文件夹复制到你 Hexo 的 <code>themes</code> 文件夹中即可。</p><p>当然你也可以在你的 <code>themes</code> 文件夹下使用 <code>Git clone</code> 命令来下载:</p><pre><code class="bash">git clone https://github.com/blinkfox/hexo-theme-matery.git</code></pre><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="切换主题"><a href="#切换主题" class="headerlink" title="切换主题"></a>切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的  <code>theme</code> 的值：<code>theme: hexo-theme-matery</code></p><h4 id="config-yml-文件的其它修改建议"><a href="#config-yml-文件的其它修改建议" class="headerlink" title="_config.yml 文件的其它修改建议:"></a><code>_config.yml</code> 文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主 <code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code> 的倍数，如：<code>12</code>、<code>18</code> 等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为 <code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页"><a href="#新建分类-categories-页" class="headerlink" title="新建分类 categories 页"></a>新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>categories/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre><code class="bash">hexo new page &quot;categories&quot;</code></pre><p>编辑你刚刚新建的页面文件 <code>/source/categories/index.md</code>，至少需要以下内容：</p><pre><code class="yaml">---title: categoriesdate: 2018-09-30 17:25:30type: &quot;categories&quot;layout: &quot;categories&quot;---</code></pre><h3 id="新建标签-tags-页"><a href="#新建标签-tags-页" class="headerlink" title="新建标签 tags 页"></a>新建标签 tags 页</h3><p><code>tags</code> 页是用来展示所有标签的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>tags/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre><code class="bash">hexo new page &quot;tags&quot;</code></pre><p>编辑你刚刚新建的页面文件 <code>/source/tags/index.md</code>，至少需要以下内容：</p><pre><code class="yaml">---title: tagsdate: 2018-09-30 18:23:38type: &quot;tags&quot;layout: &quot;tags&quot;---</code></pre><h3 id="新建关于我-about-页"><a href="#新建关于我-about-页" class="headerlink" title="新建关于我 about 页"></a>新建关于我 about 页</h3><p><code>about</code> 页是用来展示<strong>关于我和我的博客</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>about/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre><code class="bash">hexo new page &quot;about&quot;</code></pre><p>编辑你刚刚新建的页面文件 <code>/source/about/index.md</code>，至少需要以下内容：</p><pre><code class="yaml">---title: aboutdate: 2018-09-30 17:25:30type: &quot;about&quot;layout: &quot;about&quot;---</code></pre><h3 id="新建友情连接-friends-页（可选的）"><a href="#新建友情连接-friends-页（可选的）" class="headerlink" title="新建友情连接 friends 页（可选的）"></a>新建友情连接 friends 页（可选的）</h3><p><code>friends</code> 页是用来展示<strong>友情连接</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>friends/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre><code class="bash">hexo new page &quot;friends&quot;</code></pre><p>编辑你刚刚新建的页面文件 <code>/source/friends/index.md</code>，至少需要以下内容：</p><pre><code class="yaml">---title: friendsdate: 2018-12-12 21:25:30type: &quot;friends&quot;layout: &quot;friends&quot;---</code></pre><p>同时，在你的博客 <code>source</code> 目录下新建 <code>_data</code> 目录，在 <code>_data</code> 目录中新建 <code>friends.json</code> 文件，文件内容如下所示：</p><pre><code class="json">[&#123;    &quot;avatar&quot;: &quot;http://image.luokangyuan.com/1_qq_27922023.jpg&quot;,    &quot;name&quot;: &quot;码酱&quot;,    &quot;introduction&quot;: &quot;我不是大佬，只是在追寻大佬的脚步&quot;,    &quot;url&quot;: &quot;http://luokangyuan.com/&quot;,    &quot;title&quot;: &quot;前去学习&quot;&#125;, &#123;    &quot;avatar&quot;: &quot;http://image.luokangyuan.com/4027734.jpeg&quot;,    &quot;name&quot;: &quot;闪烁之狐&quot;,    &quot;introduction&quot;: &quot;编程界大佬，技术牛，人还特别好，不懂的都可以请教大佬&quot;,    &quot;url&quot;: &quot;https://blinkfox.github.io/&quot;,    &quot;title&quot;: &quot;前去学习&quot;&#125;, &#123;    &quot;avatar&quot;: &quot;http://image.luokangyuan.com/avatar.jpg&quot;,    &quot;name&quot;: &quot;ja_rome&quot;,    &quot;introduction&quot;: &quot;平凡的脚步也可以走出伟大的行程&quot;,    &quot;url&quot;: &quot;ttps://me.csdn.net/jlh912008548&quot;,    &quot;title&quot;: &quot;前去学习&quot;&#125;]</code></pre><h3 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h3><p>由于 Hexo 自带的代码高亮主题显示不好看，所以主题中使用到了 <a href="https://github.com/ele828/hexo-prism-plugin">hexo-prism-plugin</a> 的 Hexo 插件来做代码高亮，安装命令如下：</p><pre><code class="bash">npm i -S hexo-prism-plugin</code></pre><p>然后，修改 Hexo 根目录下 <code>_config.yml</code> 文件中 <code>highlight.enable</code> 的值为 <code>false</code>，并新增 <code>prism</code> 插件相关的配置，主要配置如下：</p><pre><code class="yaml">highlight:  enable: falseprism_plugin:  mode: &#39;preprocess&#39;    # realtime/preprocess  theme: &#39;tomorrow&#39;  line_number: false    # default false  custom_css:</code></pre><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>本主题中还使用到了 <a href="https://github.com/wzpan/hexo-generator-search">hexo-generator-search</a> 的 Hexo 插件来做内容搜索，安装命令如下：</p><pre><code class="bash">npm install hexo-generator-search --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre><code class="yaml">search:  path: search.xml  field: post</code></pre><h3 id="中文链接转拼音（可选的）"><a href="#中文链接转拼音（可选的）" class="headerlink" title="中文链接转拼音（可选的）"></a>中文链接转拼音（可选的）</h3><p>如果你的文章名称是中文的，那么 Hexo 默认生成的永久链接也会有中文，这样不利于 <code>SEO</code>，且 <code>gitment</code> 评论对中文链接也不支持。我们可以用 <a href="https://github.com/viko16/hexo-permalink-pinyin">hexo-permalink-pinyin</a> Hexo 插件使在生成文章时生成中文拼音的永久链接。</p><p>安装命令如下：</p><pre><code class="bash">npm i hexo-permalink-pinyin --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre><code class="yaml">permalink_pinyin:  enable: true  separator: &#39;-&#39; # default: &#39;-&#39;</code></pre><blockquote><p><strong>注</strong>：除了此插件外，<a href="https://github.com/rozbo/hexo-abbrlink">hexo-abbrlink</a> 插件也可以生成非中文的链接。</p></blockquote><h3 id="文章字数统计插件（可选的）"><a href="#文章字数统计插件（可选的）" class="headerlink" title="文章字数统计插件（可选的）"></a>文章字数统计插件（可选的）</h3><p>如果你想要在文章中显示文章字数、阅读时长信息，可以安装 <a href="https://github.com/willin/hexo-wordcount">hexo-wordcount</a>插件。</p><p>安装命令如下：</p><pre><code class="bash">npm i --save hexo-wordcount</code></pre><p>然后只需在本主题下的 <code>_config.yml</code> 文件中，激活以下配置项即可：</p><pre><code class="yaml">wordCount:  enable: false # 将这个值设置为 true 即可.  postWordCount: true  min2read: true  totalCount: true</code></pre><h3 id="添加-RSS-订阅支持（可选的）"><a href="#添加-RSS-订阅支持（可选的）" class="headerlink" title="添加 RSS 订阅支持（可选的）"></a>添加 RSS 订阅支持（可选的）</h3><p>本主题中还使用到了 <a href="https://github.com/hexojs/hexo-generator-feed">hexo-generator-feed</a> 的 Hexo 插件来做 <code>RSS</code>，安装命令如下：</p><pre><code class="bash">npm install hexo-generator-feed --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre><code class="yaml">feed:  type: atom  path: atom.xml  limit: 20  hub:  content:  content_limit: 140  content_limit_delim: &#39; &#39;  order_by: -date</code></pre><p>执行 <code>hexo clean &amp;&amp; hexo g</code> 重新生成博客文件，然后在 <code>public</code> 文件夹中即可看到 <code>atom.xml</code> 文件，说明你已经安装成功了。</p><h3 id="修改页脚"><a href="#修改页脚" class="headerlink" title="修改页脚"></a>修改页脚</h3><p>页脚信息可能需要做定制化修改，而且它不便于做成配置信息，所以可能需要你自己去再修改和加工。修改的地方在主题文件的 <code>/layout/_partial/footer.ejs</code> 文件中，包括站点、使用的主题、访问量等。</p><h3 id="修改社交链接"><a href="#修改社交链接" class="headerlink" title="修改社交链接"></a>修改社交链接</h3><p>在主题的 <code>_config.yml</code> 文件中，默认支持 <code>QQ</code>、<code>GitHub</code> 和邮箱的配置，你可以在主题文件的 <code>/layout/_partial/social-link.ejs</code> 文件中，新增、修改你需要的社交链接地址，增加链接可参考如下代码：</p><pre><code class="html">&lt;a href=&quot;https://github.com/blinkfox&quot; class=&quot;tooltipped&quot; target=&quot;_blank&quot; data-tooltip=&quot;访问我的GitHub&quot; data-position=&quot;top&quot; data-delay=&quot;50&quot;&gt;    &lt;i class=&quot;fa fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;</code></pre><p>其中，社交图标（如：<code>fa-github</code>）你可以在 <a href="https://fontawesome.com/icons">Font Awesome</a> 中搜索找到。以下是常用社交图标的标识，供你参考：</p><ul><li>Facebook: <code>fa-facebook</code></li><li>Twitter: <code>fa-twitter</code></li><li>Google-plus: <code>fa-google-plus</code></li><li>Linkedin: <code>fa-linkedin</code></li><li>Tumblr: <code>fa-tumblr</code></li><li>Medium: <code>fa-medium</code></li><li>Slack: <code>fa-slack</code></li><li>新浪微博: <code>fa-weibo</code></li><li>微信: <code>fa-wechat</code></li><li>QQ: <code>fa-qq</code></li></ul><blockquote><p><strong>注意</strong>: 本主题中使用的 <code>Font Awesome</code> 版本为 <code>4.7.0</code>。</p></blockquote><h3 id="修改打赏的二维码图片"><a href="#修改打赏的二维码图片" class="headerlink" title="修改打赏的二维码图片"></a>修改打赏的二维码图片</h3><p>在主题文件的 <code>source/medias/reward</code> 文件中，你可以替换成你的的微信和支付宝的打赏二维码图片。</p><h3 id="配置音乐播放器（可选的）"><a href="#配置音乐播放器（可选的）" class="headerlink" title="配置音乐播放器（可选的）"></a>配置音乐播放器（可选的）</h3><p>要支持音乐播放，就必须开启音乐的播放配置和音乐数据的文件。</p><p>首先，在你的博客 <code>source</code> 目录下的 <code>_data</code> 目录（没有的话就新建一个）中新建 <code>musics.json</code> 文件，文件内容如下所示：</p><pre><code class="json">[&#123;    &quot;name&quot;: &quot;五月雨变奏电音&quot;,    &quot;artist&quot;: &quot;AnimeVibe&quot;,    &quot;url&quot;: &quot;http://xxx.com/music1.mp3&quot;,    &quot;cover&quot;: &quot;http://xxx.com/music-cover1.png&quot;&#125;, &#123;    &quot;name&quot;: &quot;Take me hand&quot;,    &quot;artist&quot;: &quot;DAISHI DANCE,Cecile Corbel&quot;,    &quot;url&quot;: &quot;/medias/music/music2.mp3&quot;,    &quot;cover&quot;: &quot;/medias/music/cover2.png&quot;&#125;, &#123;    &quot;name&quot;: &quot;Shape of You&quot;,    &quot;artist&quot;: &quot;J.Fla&quot;,    &quot;url&quot;: &quot;http://xxx.com/music3.mp3&quot;,    &quot;cover&quot;: &quot;http://xxx.com/music-cover3.png&quot;&#125;]</code></pre><blockquote><p><strong>注</strong>：以上 JSON 中的属性：<code>name</code>、<code>artist</code>、<code>url</code>、<code>cover</code> 分别表示音乐的名称、作者、音乐文件地址、音乐封面。</p></blockquote><p>然后，在主题的 <code>_config.yml</code> 配置文件中激活配置即可：</p><pre><code class="yaml"># 是否在首页显示音乐.music:  enable: true  showTitle: false  title: 听听音乐  fixed: false # 是否开启吸底模式  autoplay: false # 是否自动播放  theme: &#39;#42b983&#39;  loop: &#39;all&#39; # 音频循环播放, 可选值: &#39;all&#39;, &#39;one&#39;, &#39;none&#39;  order: &#39;list&#39; # 音频循环顺序, 可选值: &#39;list&#39;, &#39;random&#39;  preload: &#39;auto&#39; # 预加载，可选值: &#39;none&#39;, &#39;metadata&#39;, &#39;auto&#39;  volume: 0.7 # 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效  listFolded: false # 列表默认折叠  listMaxHeight: # 列表最大高度</code></pre><h2 id="文章-Front-matter-介绍"><a href="#文章-Front-matter-介绍" class="headerlink" title="文章 Front-matter 介绍"></a>文章 Front-matter 介绍</h2><h3 id="Front-matter-选项详解"><a href="#Front-matter-选项详解" class="headerlink" title="Front-matter 选项详解"></a>Front-matter 选项详解</h3><p><code>Front-matter</code> 选项中的所有内容均为<strong>非必填</strong>的。但我仍然建议至少填写 <code>title</code> 和 <code>date</code> 的值。</p><table><thead><tr><th>配置选项</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>title</td><td><code>Markdown</code> 的文件标题</td><td>文章标题，强烈建议填写此选项</td></tr><tr><td>date</td><td>文件创建时的日期时间</td><td>发布时间，强烈建议填写此选项，且最好保证全局唯一</td></tr><tr><td>author</td><td>根 <code>_config.yml</code> 中的 <code>author</code></td><td>文章作者</td></tr><tr><td>img</td><td><code>featureImages</code> 中的某个值</td><td>文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径.如: <code>http://xxx.com/xxx.jpg</code></td></tr><tr><td>top</td><td><code>true</code></td><td>推荐文章（文章是否置顶），如果 <code>top</code> 值为 <code>true</code>，则会作为首页推荐文章</td></tr><tr><td>cover</td><td><code>false</code></td><td><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td></tr><tr><td>coverImg</td><td>无</td><td><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td></tr><tr><td>password</td><td>无</td><td>文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置 <code>password</code> 的值，该值必须是用 <code>SHA256</code> 加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code> 中激活了 <code>verifyPassword</code> 选项</td></tr><tr><td>toc</td><td><code>true</code></td><td>是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的 <code>config.yml</code> 中激活了 <code>toc</code> 选项</td></tr><tr><td>mathjax</td><td><code>false</code></td><td>是否开启数学公式支持 ，本文章是否开启 <code>mathjax</code>，且需要在主题的 <code>_config.yml</code> 文件中也需要开启才行</td></tr><tr><td>summary</td><td>无</td><td>文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td></tr><tr><td>categories</td><td>无</td><td>文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td></tr><tr><td>tags</td><td>无</td><td>文章标签，一篇文章可以多个标签</td></tr><tr><td>reprintPolicy</td><td>cc_by</td><td>文章转载规则， 可以是 cc_by, cc_by_nd, cc_by_sa, cc_by_nc, cc_by_nc_nd, cc_by_nc_sa, cc0, noreprint 或 pay 中的一个</td></tr></tbody></table><blockquote><p><strong>注意</strong>:</p><ol><li>如果 <code>img</code> 属性不填写的话，文章特色图会根据文章标题的 <code>hashcode</code> 的值取余，然后选取主题中对应的特色图片，从而达到让所有文章都的特色图<strong>各有特色</strong>。</li><li><code>date</code> 的值尽量保证每篇文章是唯一的，因为本主题中 <code>Gitalk</code> 和 <code>Gitment</code> 识别 <code>id</code> 是通过 <code>date</code> 的值来作为唯一标识的。</li><li>如果要对文章设置阅读验证密码的功能，不仅要在 Front-matter 中设置采用了 SHA256 加密的 password 的值，还需要在主题的 <code>_config.yml</code> 中激活了配置。有些在线的 SHA256 加密的地址，可供你使用：<a href="http://tool.oschina.net/encrypt?type=2">开源中国在线工具</a>、<a href="http://encode.chahuo.com/">chahuo</a>、<a href="http://tool.chinaz.com/tools/hash.aspx">站长工具</a>。</li><li>您可以在文章md文件的 front-matter 中指定 reprintPolicy 来给单个文章配置转载规则</li></ol></blockquote><p>以下为文章的 <code>Front-matter</code> 示例。</p><h3 id="最简示例"><a href="#最简示例" class="headerlink" title="最简示例"></a>最简示例</h3><pre><code class="yaml">---title: typora-vue-theme主题介绍date: 2018-09-07 09:25:00---</code></pre><h3 id="最全示例"><a href="#最全示例" class="headerlink" title="最全示例"></a>最全示例</h3><pre><code class="yaml">---title: typora-vue-theme主题介绍date: 2018-09-07 09:25:00author: 赵奇img: /source/images/xxx.jpgtop: truecover: truecoverImg: /images/1.jpgpassword: 8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92toc: falsemathjax: falsesummary: 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要categories: Markdowntags:  - Typora  - Markdown---</code></pre><h2 id="效果截图"><a href="#效果截图" class="headerlink" title="效果截图"></a>效果截图</h2><p><img src="http://static.blinkfox.com/matery-20181202-1.png" alt="首页"></p><p><img src="http://static.blinkfox.com/matery-20181202-2.png" alt="首页推荐文章"></p><p><img src="http://static.blinkfox.com/matery-20181202-3.png" alt="首页文章列表"></p><p><img src="http://static.blinkfox.com/matery-20181202-7.png" alt="首页文章列表"></p><p><img src="http://static.blinkfox.com/matery-20181202-8.png" alt="首页文章列表"></p><h2 id="自定制修改"><a href="#自定制修改" class="headerlink" title="自定制修改"></a>自定制修改</h2><p>在本主题的 <code>_config.yml</code> 中可以修改部分自定义信息，有以下几个部分：</p><ul><li>菜单</li><li>我的梦想</li><li>首页的音乐播放器和视频播放器配置</li><li>是否显示推荐文章名称和按钮配置</li><li><code>favicon</code> 和 <code>Logo</code></li><li>个人信息</li><li>TOC 目录</li><li>文章打赏信息</li><li>复制文章内容时追加版权信息</li><li>MathJax</li><li>文章字数统计、阅读时长</li><li>点击页面的’爱心’效果</li><li>我的项目</li><li>我的技能</li><li>我的相册</li><li><code>Gitalk</code>、<code>Gitment</code>、<code>Valine</code> 和 <code>disqus</code> 评论配置</li><li><a href="http://busuanzi.ibruce.info/">不蒜子统计</a>和谷歌分析（<code>Google Analytics</code>）</li><li>默认特色图的集合。当文章没有设置特色图时，本主题会根据文章标题的 <code>hashcode</code> 值取余，来选择展示对应的特色图</li></ul><p><strong>我认为个人博客应该都有自己的风格和特色</strong>。如果本主题中的诸多功能和主题色彩你不满意，可以在主题中自定义修改，很多更自由的功能和细节点的修改难以在主题的 <code>_config.yml</code> 中完成，需要修改源代码才来完成。以下列出了可能对你有用的地方：</p><h3 id="修改主题颜色"><a href="#修改主题颜色" class="headerlink" title="修改主题颜色"></a>修改主题颜色</h3><p>在主题文件的 <code>/source/css/matery.css</code> 文件中，搜索 <code>.bg-color</code> 来修改背景颜色：</p><pre><code class="css">/* 整体背景颜色，包括导航、移动端的导航、页尾、标签页等的背景颜色. */.bg-color &#123;    background-image: linear-gradient(to right, #4cbf30 0%, #0f9d58 100%);&#125;@-webkit-keyframes rainbow &#123;   /* 动态切换背景颜色. */&#125;@keyframes rainbow &#123;    /* 动态切换背景颜色. */&#125;</code></pre><h3 id="修改-banner-图和文章特色图"><a href="#修改-banner-图和文章特色图" class="headerlink" title="修改 banner 图和文章特色图"></a>修改 banner 图和文章特色图</h3><p>你可以直接在 <code>/source/medias/banner</code> 文件夹中更换你喜欢的 <code>banner</code> 图片，主题代码中是每天动态切换一张，只需 <code>7</code> 张即可。如果你会 <code>JavaScript</code> 代码，可以修改成你自己喜欢切换逻辑，如：随机切换等，<code>banner</code> 切换的代码位置在 <code>/layout/_partial/bg-cover-content.ejs</code> 文件的 <code>&lt;script&gt;&lt;/script&gt;</code> 代码中：</p><pre><code class="javascript">$(&#39;.bg-cover&#39;).css(&#39;background-image&#39;, &#39;url(/medias/banner/&#39; + new Date().getDay() + &#39;.jpg)&#39;);</code></pre><p>在 <code>/source/medias/featureimages</code> 文件夹中默认有 24 张特色图片，你可以再增加或者减少，并需要在 <code>_config.yml</code> 做同步修改。</p><h2 id="版本记录"><a href="#版本记录" class="headerlink" title="版本记录"></a>版本记录</h2><ul><li>v1.0.4<ul><li>新增了能为每篇文章都自定义转载规则的功能；</li><li>修复上一页、下一页的自定义 <code>summary</code> 不显示的问题；</li><li>修复了友情链接显示错位的问题，改为了瀑布流的布局方式；</li><li>其他小细节 bug 的修改；</li></ul></li><li>v1.0.3<ul><li>新增了<code>TOC</code>展开、收缩的按钮和相关配置，默认显示此按钮；</li></ul></li><li>v1.0.2<ul><li>升级了 <a href="https://materializecss.com/">Materialize</a> 框架版本为<code>1.0.0</code>，重构和修改了升级过程中的部分文件或问题；</li><li>新增了首页封面的全屏轮播特效，可以将更重要的文章设置到首页轮播中；</li><li>修复首页第一个按钮是中文的问题</li><li>修复了 iPhone 上点击搜索输入获取焦点的问题；</li><li>修复了 iPhone 上输入框获取焦点后页面放大的问题；</li><li>修复一些文章或 UI 显示问题；</li></ul></li><li>v1.0.1<ul><li>调整 <code>css</code>、<code>js</code> 的文件请求路径在主题的<code>_config.yml</code>中配置，便于你更快捷的配置自己的 CDN；</li><li>新增代码是否折行为可配置，默认为折行；</li><li>默认激活 <code>TOC</code> 功能，并新增为某篇文章关闭 <code>TOC</code> 的 <code>Front-matter</code> 配置选项；</li><li>修复文章滚动时，高亮的目录选项不准确的问题；</li><li><code>IOS</code>下移除搜索框自动获得焦点属性，防止自动获得焦点后导致视图上移；</li></ul></li><li>v1.0.0<ul><li>新增了所有基础功能；</li></ul></li></ul>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h1 id=&quot;hexo-theme-matery-主题说明文档&quot;&gt;&lt;a href=&quot;#hexo-theme-matery-主题说明文档&quot; class=&quot;headerlink&quot; title=&quot;hexo-theme-matery</summary>
        
      
    
    
    
    <category term="Tutorial" scheme="https://ukeyboard.github.io/categories/Tutorial/"/>
    
    
    <category term="Website" scheme="https://ukeyboard.github.io/tags/Website/"/>
    
    <category term="Hexo" scheme="https://ukeyboard.github.io/tags/Hexo/"/>
    
    <category term="Theme" scheme="https://ukeyboard.github.io/tags/Theme/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客建站指南</title>
    <link href="https://ukeyboard.github.io/2019/06/13/how-to-use-hexo/"/>
    <id>https://ukeyboard.github.io/2019/06/13/how-to-use-hexo/</id>
    <published>2019-06-13T12:07:37.000Z</published>
    <updated>2019-06-14T00:00:25.591Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Hexo-博客建站指南"><a href="#Hexo-博客建站指南" class="headerlink" title="Hexo 博客建站指南"></a>Hexo 博客建站指南</h3><blockquote><p>写在前面：</p></blockquote><h4 id="安装篇"><a href="#安装篇" class="headerlink" title="安装篇"></a>安装篇</h4><p>Hexo依赖于 nodejs 和 git， 先安装依赖文件。</p><pre><code class="bash"># For mac with brew$ brew install git$ brew install node</code></pre><p>然后安装 <code>hexo-cli</code></p><pre><code class="bash">$ npm install -g hexo-cli</code></pre><p>安装好后我们就可以使用 <code>hexo</code> 命令来建站、添加内容、生成静态文件，但还不能本地预览和远程发布。本地预览需要先安装服务器<code>hexo-server</code>；远程发布需要安装部署服务（deployer）。</p><h4 id="初探"><a href="#初探" class="headerlink" title="初探"></a>初探</h4><p>现在我们开始建第一个网站：</p><pre><code class="bash">$ hexo init me # me为站点目录，用于存放hexo相关初始化文件，用户可指定任意其他名称</code></pre><p>该命令会在当前目录下新建 <code>me</code> 文件夹，并下载hexo需要的初始化文件到改目录。新建完成后，指定目录内容如下：</p><pre><code>.├── _config.yml├── package.json├── scaffolds├── source|   ├── _drafts|   └── _posts└── themes</code></pre><p>各文件和目录的含义见：<a href="https://hexo.io/zh-cn/docs/setup">Hexo文档 — 建站</a></p><p>下面新建我们的第一个博客文档：<code>hexo new &quot;how to use hexo&quot;</code> , 该命令会在目录<code>source/_post</code> 下新建 <code>how-to-use-hexo.md</code> 文件。<em>注意：新建的makrdown文档命名时考虑到了同名冲突问题，用户可自行测试。</em></p><p>通过 <code>hexo new</code>命令新建的markdown文件现处于纯文档状态，没有经过渲染和静态化处理。我们可以使用 <code>hexo generate</code> 或者简写形式 <code>hexo g</code> 使用主题渲染markdown文件、生成html静态文件。该命令会在站点目录下新建 <code>public</code> 目录并按规则将渲染后的html静态文件放置在该目录相应的子目录下，同时准备好其他 js脚本文件和css样式文件。</p><pre><code>public/├── 2019├── archives├── css├── fancybox├── index.html└── js</code></pre><p>渲染markdown文件使用的默认主题在 <code>_config.yml</code> 中定义：</p><pre><code class="ymal"># Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape</code></pre><p>到目前为止，Hexo已经为我们准备好了静态化的经渲染的页面，我们可以将public目录下的文件发布（通过hexo自带的发布器或者直接拷贝）到服务器上对外展示。我们也可以启动 hexo server 开启本地预览服务器，关于本地预览服务器查看服务器篇。</p><h4 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h4><ol><li><a href="https://hexo.io/zh-cn/docs/">Hexo官方文档</a></li><li><a href="https://www.jianshu.com/p/84a8384be1ae">Hexo使用指南</a></li><li><a href="https://segmentfault.com/a/1190000010680022">Hexo 个人博客部署到 CentOS 个人服务器</a></li><li><a href="https://www.simon96.online/2018/10/12/hexo-tutorial/">最全Hexo博客搭建</a></li><li><a href="%5Bhttp://jayveehe.github.io/2014/11/08/hexo-question-summarize/%5D(http://jayveehe.github.io/2014/11/08/hexo-question-summarize/)">用Hexo建立Github Page——你可能会遇到的问题</a></li><li><a href="https://blog.csdn.net/qq_21808961/article/details/84476504">hexo d命令报错 ERROR Deployer not found: git</a></li></ol>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h3 id=&quot;Hexo-博客建站指南&quot;&gt;&lt;a href=&quot;#Hexo-博客建站指南&quot; class=&quot;headerlink&quot; title=&quot;Hexo 博客建站指南&quot;&gt;&lt;/a&gt;Hexo</summary>
        
      
    
    
    
    <category term="Tutorial" scheme="https://ukeyboard.github.io/categories/Tutorial/"/>
    
    
    <category term="Website" scheme="https://ukeyboard.github.io/tags/Website/"/>
    
  </entry>
  
  <entry>
    <title>Git命令使用手册</title>
    <link href="https://ukeyboard.github.io/2019/04/13/how-to-use-git/"/>
    <id>https://ukeyboard.github.io/2019/04/13/how-to-use-git/</id>
    <published>2019-04-13T12:07:37.000Z</published>
    <updated>2019-07-03T02:06:10.703Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Git-使用手册"><a href="#Git-使用手册" class="headerlink" title="Git 使用手册"></a>Git 使用手册</h2><p>Git是非常流行的版本控制软件，本篇带领大家学习如何快速使用Git管理自己的代码。</p><h3 id="基础使用篇"><a href="#基础使用篇" class="headerlink" title="基础使用篇"></a>基础使用篇</h3><h4 id="Git配置"><a href="#Git配置" class="headerlink" title="Git配置"></a>Git配置</h4><p>Git有几个重要的配置文件：</p><table><thead><tr><th>配置文件</th><th>说明信息</th></tr></thead><tbody><tr><td>/etc/gitconfig</td><td>system级别的配置信息，通过 git config –system 读写</td></tr><tr><td><del>/.gitconfig<br /></del>/.config/git/config</td><td>用户全局配置文件，通过 git config –global 读写</td></tr><tr><td>.git/config</td><td>用户特定项目配置文件，通过 git config –local 或  git config 直接读写</td></tr></tbody></table><p>配置信息按照 system -&gt; 用户全局 -&gt; 用户特定项目 的顺序覆盖，即用户全局要比system级别配置信息优先级高，对于项目而言，用户特定项目配置具有最高优先级。</p><h5 id="添加身份信息"><a href="#添加身份信息" class="headerlink" title="添加身份信息"></a>添加身份信息</h5><p>开始使用Git前，让我们先设置身份识别信息，通过</p><pre><code class="bash">git config [--global] user.name &quot;YourUserName&quot;git config [--global] user.email &quot;YourUserEmail&quot;</code></pre><p>告诉Git你是谁。添加 <code>--global</code> 参数意在指示Git为所有项目使用同一套身份识别信息。如果你想为特定的项目使用特定的身份识别信息，只需在项目的根目录不带参 <code>--global</code> 设置身份信息即可。身份信息可以在任何时候进行更改。设置项目特定身份识别信息时，请确认项目是已初始化的Git项目。</p><p>Git将使用身份信息修饰每一个commit，你提交的commit中包含个人身份信息，这主要作为一种溯源的手段，方便查看是谁提交了commit、做了哪些修改，实际上Git并不检查你设置的身份信息是否真实有效。</p><p><strong>GitHub</strong>：如果要结合GitHub使用Git，GitHub需要进行额外的配置。Git与GitHub的交互是通过GitHub中设置的主邮箱进行的，所有为了与Github交互你可以将用于标记commit的邮箱添加为Github信任邮箱并设置为主邮箱，也可以将Github的主邮箱设置为身份识别邮箱。Git提交的commit包含身份识别信息，而commit又在多用户之间共享，如果你不想暴露自己的身份身份信息，Github提供保护隐私的手段，你可以在Github中配置隐藏私人邮箱并使用Github提供的 <code>noreply</code> 邮箱作为交互通行证。</p><p>默认情况下，你可以在Github中指定任意已认证邮箱为主邮箱用于交互。但如果启用的邮箱隐私保护，那么你只能通过Github <code>noreply</code> 邮箱来提交commits. 另外Git <code>user.name</code> 和 Github用户名不是一回事情。</p><p><strong>参考资料：</strong></p><ul><li><p><a href="https://help.github.com/articles/about-commit-email-addresses/">About commit email addresses</a></p></li><li><p><a href="https://help.github.com/articles/setting-your-username-in-git/">Setting your username in Git</a></p></li><li><p><a href="https://help.github.com/articles/setting-your-commit-email-address-in-git/">Setting your commit email address in Git</a></p></li><li><p><a href="https://help.github.com/articles/setting-your-commit-email-address-on-github/">Setting your commit email address on GitHub</a></p></li><li><p><a href="https://help.github.com/articles/blocking-command-line-pushes-that-expose-your-personal-email-address/">Blocking command line pushes that expose your personal email address</a></p></li></ul><h3 id="进阶使用篇"><a href="#进阶使用篇" class="headerlink" title="进阶使用篇"></a>进阶使用篇</h3><h4 id="Git私有云"><a href="#Git私有云" class="headerlink" title="Git私有云"></a>Git私有云</h4><p>在Github还不允许建立私有仓库的时候，如何在本地搭建一个私有仓库来管理不希望开源的代码是一个很有用方案，待Github允许私有仓库后搭建本地私有仓库看似没有必要了，但是考虑到本地仓库管理方便、同步速度快、没有网络要求等优势，本节将继续介绍如何搭建本地仓库管理代码以及如何结合其他云平台同步工具实现私有云。</p><p> <strong>参考资料</strong>：</p><ul><li><a href="https://chuyao.github.io/2017/11/17/git-usb/">在U盘上建立git仓库，移动的“私有云”</a></li><li><a href="http://ibruce.info/2013/12/30/git-with-dropbox/">使用Dropbox建立Git私有仓库</a></li><li><a href="http://fatmouse.xyz/2014/12/15/2014-12-15-shi-yong-wang-pan-jian-zao-gitsi-you-cang-ku/">使用网盘建造Git私有仓库</a></li></ul>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h2 id=&quot;Git-使用手册&quot;&gt;&lt;a href=&quot;#Git-使用手册&quot; class=&quot;headerlink&quot; title=&quot;Git 使用手册&quot;&gt;&lt;/a&gt;Git</summary>
        
      
    
    
    
    <category term="Tutorial" scheme="https://ukeyboard.github.io/categories/Tutorial/"/>
    
    
    <category term="Git" scheme="https://ukeyboard.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>DropBlock面向卷积网络的新正则化方法</title>
    <link href="https://ukeyboard.github.io/2019/04/05/DropBlock/"/>
    <id>https://ukeyboard.github.io/2019/04/05/DropBlock/</id>
    <published>2019-04-05T12:07:37.000Z</published>
    <updated>2019-06-15T10:52:38.629Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DropBlock-面向卷积网络的新正则化方法"><a href="#DropBlock-面向卷积网络的新正则化方法" class="headerlink" title="DropBlock: 面向卷积网络的新正则化方法"></a>DropBlock: 面向卷积网络的新正则化方法</h2><p>近日arXiv上发表一篇新的关于卷积网络训练的文章<strong>DropBlock: A regularization method for convolutional networks</strong>，文章来自鼎鼎大名的<a href="https://ai.google/research/teams/brain">Google Brain</a>，目前已经被<a href="https://nips.cc/">NIPS 2018</a>收录为Poster文章。</p><p><img src="https://i.postimg.cc/BvGrjf2t/1540967782364.png" alt="1540967782364"></p><h3 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h3><blockquote><p>机器学习的主要主要目的是用有限的标注数据来训练模型，使得模型在没有见过的数据上也能表现良好，即模型泛化能力强。例如：对于一个识别猫的模型，我们给它一张猫的图片并告知图中是猫，那么模型下次在见到猫的图片时也能判断出图中是猫，即使这个猫和它之前见过的不是同一只。</p></blockquote><p>我们将用于训练模型的数据称为<strong>训练集</strong>，将用于模型测试的数据称为<strong>测试集</strong>，我们希望模型在训练集和测试集上的表现是一致的，模型在训练集上表现好则在测试集上也要表现好才行。这里一个重要的前提条件是：<strong>训练集和测试集的数据是从同一个分布中采样生成的</strong>。</p><p>如果将训练数据和测试数据看成空间中的点，那么模型就是空间中的函数，函数的性质由任务而定，比如分类则要求函数能够把点划分到不同的集合就好，而回归则要求函数要经过所有点。不管哪种特定任务，这样的函数并不唯一，例如：平面上经过两个点的函数有无限多，我们的目的则是从这些函数组成的函数族中找到一个能够较好地拟合数据分布的函数。找函数的过程就是进行模型训练的过程，如果训练数据不够多或者模型太复杂，模型容易过拟合，表现出来就是在训练集上表现完美，但在测试集上表现就不那么理想。</p><p><em>关于模型过拟合的问题在《模式识别和机器学习》中有比较详细的介绍。</em></p><p>今天的内容就与缓解模型过拟合、提高泛化能力的有关。</p><h3 id="二、相关工作"><a href="#二、相关工作" class="headerlink" title="二、相关工作"></a>二、相关工作</h3><p>深度神经网络含有大量需要训练的参数，从数学上理解它是一个极其复杂的函数，一般为了使用有限的训练数据对网络进行训练，我们会引入大量的噪声和正则约束，比如weight decay 和 dropout。其中dropout在AlexNet中被提出来用于图像分类，虽然最初dropout是和卷积层一起使用的，近年来却很少和卷积层一起出现了，多被用于全连接层激活函数值的随机舍弃。</p><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>Dropout的原理很好理解：让神经元以一定概率停止工作，实现上则是将神经元的激活结果按  $p$  设成 $0$, 公式如下：<br>$$<br>r_{j}^{l} \thicksim Bernoulli(p)  \<br>\vec{\tilde{y}} = \vec{r}^l * \vec{y}^l<br>$$<br>其中 $ \vec{y}^l$ 为第$l$层的激活值， $\vec{r}^l$ 是该层使用的dropout mask值，$\vec{y}^l$和$\vec{r}^l$有相同的分辨率或大小。这种随机舍弃的策略可以提高模型的泛化能力，在后来的模型设计中被广泛使用。一种说法认为Dropout是一种模型平均策略，模型的每一个训练iter中因为随机舍弃了部分激活信息使得模型发生变化，训练使用dropout的模型实际上是在训练很多网络结构相同、权值共享的模型。</p><blockquote><p>Dropout provides an inexpensive and simple means of both training a large assemble of models that share parameters and approximately averaging together these models’ predictions.</p><p>Dropout is generally viewed as an indiscriminately applicable tool that reliable yields a modest improvement in performance when applied to almost any model.</p></blockquote><h4 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h4><p>13年蒙特利尔大学在ICML上发表论文 <a href="https://arxiv.org/pdf/1302.4389.pdf">《Maxout Networks》</a> 认为dropout是在用训练集的不同子集来训练不同的子模型（sub-model），所有子模型共享参数。训练过程像是在训练一个包含很多分支的ensemble网络，区别是所有分支权值共享。问题是如何merge这些子模型的预测值来得到最终预测？</p><p>我们先从数学上理解一下：假设网络包含$L$个隐藏层$\mathbf{h}={h_1, h_2, …, h_L}$，网络参数为$\theta$，网络输入$v$对应Label为$y$，假设隐藏层包含$N$个神经元，那么在每个隐藏层后应用dropout的结果是网络衍生成$\mathbf{M}=2^N$个子模型，不同的dropout mask $\mu$对应不同的子模型。$\mu$的每个元素在${0,1}$中取值，0表示舍弃，1表示保留。训练时，由于dropout随机舍弃，可以认为每个iter应用的$\mu$不同，故而子模型不同，子模型通过$log(y|v, \theta, u)$进行优化。</p><p>Dropout作为一种模型平均策略其模型平均的能力是隐含在算法实现里的，当dropout应用到深度神经网络模型的时候，由于训练过程无法穷举所有可能的舍弃情况，最终模型也只能是显式进行模型平均的一个近似估计。</p><p>为减小这种模型平均的估计误差，Maxout引入了maxout单元作为激活函数配合dropout使用:<br>$$<br>h_i(x)= \displaystyle\max_{j=1…k} z_{ij}<br>$$<br>其中$z_{ij}= x^TW_{…ij}+b_{ij}$，$x \in R^d$， $W \in R^{d \times m \times k}$，$b \in R^{m \times k}$， $W,b$ 是待学习参数。这样对每个输出层$m_i$内部都包含$k$个隐藏层，$m_i$的最终输出是在内部$k$个隐藏层中取max的结果。如果$k=1$则模型简化成我们熟知的MLP了。</p><pre><code class="python">@add_arg_scopedef maxout(inputs, num_units, axis=-1, scope=None):  &quot;&quot;&quot;Adds a maxout op from https://arxiv.org/abs/1302.4389  &quot;Maxout Networks&quot; Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron  Courville,   Yoshua Bengio  Usually the operation is performed in the filter/channel dimension. This can  also be  used after fully-connected layers to reduce number of features.  Arguments:    inputs: Tensor input    num_units: Specifies how many features will remain after maxout      in the `axis` dimension (usually channel).      This must be multiple of number of `axis`.    axis: The dimension where max pooling will be performed. Default is the    last dimension.    scope: Optional scope for variable_scope.  Returns:    A `Tensor` representing the results of the pooling operation.  Raises:    ValueError: if num_units is not multiple of number of features.  &quot;&quot;&quot;  with variable_scope.variable_scope(scope, &#39;MaxOut&#39;, [inputs]):    inputs = ops.convert_to_tensor(inputs)    shape = inputs.get_shape().as_list()    num_channels = shape[axis]    if num_channels % num_units:      raise ValueError(&#39;number of features(&#123;&#125;) is not &#39;                       &#39;a multiple of num_units(&#123;&#125;)&#39;.format(                           num_channels, num_units))    shape[axis] = -1    shape += [num_channels // num_units]    # Dealing with batches with arbitrary sizes    for i in range(len(shape)):      if shape[i] is None:        shape[i] = array_ops.shape(inputs)[i]    outputs = math_ops.reduce_max(        array_ops.reshape(inputs, shape), -1, keepdims=False)    return outputs</code></pre><p>以上是<code>tf.contrib.layers.maxout</code>的实现。Maxout单元是一个分段函数，理论上任何凸函数都能通过分段函数近似表达。</p><h4 id="SpatialDropout"><a href="#SpatialDropout" class="headerlink" title="SpatialDropout"></a>SpatialDropout</h4><p>Dropout通过随机舍弃激活值的方式引入噪声，从而防止模型过拟合提高泛化能力。Yann LeCun组在论文 <a href="https://arxiv.org/pdf/1411.4280">《Efficient Object Localization Using Convolutional Network》</a>实验中发现在非$1 \times 1$卷积层应用dropout不仅使得模型训练时间大大变长，更重要的是并不能保证防止过拟合。他们认为原因主要在于卷积操作具有极强的空间关联性，因此特征图的激活值也是紧密相关的，这种情况下随机dropout一些特征单元并不能解决问题。</p><p><img src="https://www.groundai.com/media/arxiv_projects/33221/figures/standard_dropout.svg" alt="Figure 2: Standard Dropout after a 1D convolution layer"></p><p>如上图中，$F_1$、$F_2$为上一层激活值经dropout后的输出，$W_{1Pos2}, W_{1Pos1}$ 和 $W_{2Pos2}, W_{2Pos1}$分别是特征单元$f_{1a},f_{1b}$和$f_{2a},f_{2b}$上的卷积核。误差回传时，$W_2$的中间位置同时接收到从$f_{2a},f_{2b}$贡献的量，其中因为$f_{2b}$被dropout置零，$f_{2b}$的贡献也将忽略不计。本质上，这个过程可以看作是对$lr$学习率的调整，真正的学习率近似等于 $lr*p$， $1-p$ 是dropout的舍弃概率，但独立性（independence）并没有得到改善。</p><p>基于此，作者在dropout的基础上提出SpatialDropout，思路很好理解：就是在channel层面上进行随机舍弃，对一个$C \times M \times N$的特征图，如果第$c_i$个特征通道被选择舍弃，那么该通道的所有$M \times N$ 个特征单元都被置零，就像下面这样：</p><p><img src="https://www.groundai.com/media/arxiv_projects/33221/figures/our_dropout.svg" alt="Figure 4: SpatialDropout after a 1D convolution layer"></p><h3 id="三、DropBlock"><a href="#三、DropBlock" class="headerlink" title="三、DropBlock"></a>三、DropBlock</h3><p>与Dropout不适合应用于卷积层的随机舍弃和SpatialDropout激进的全通道舍弃相比，DropBlock提出了一种更加适合卷积层的dropout方式。在DropBlock中，我们随机挑选一块特征区域进行dropout，这种方式能从特征图中屏蔽一部分语义内容，从而更有效地引入噪声。引入DropBlock的动机是：dropout并不能很好地屏蔽语义内容，因为相邻特征单元之间是强相关的，舍弃的特征单元包含的语义信息在相邻区域单元中依然存在，dropout引入噪声的能力较弱。</p><p><img src="https://i.postimg.cc/L62vg4bc/1541234945894.png" alt="DropBlock和Dropout对比"></p><p>如上图中$(a)$是输入图像，$(b),(c)$ 分别是Dropout和DropBlock操作，深色方形表示包含语义信息的特征单元。Dropout屏蔽语言信息能力有限，而DropBlock屏蔽的效果更好，这样就强制网络从别的特征单元来提取特征，使得学到的网络更加鲁棒。</p><p>相比Dropout，DropBlock引入了一个新的超参$\text{block_size}$用于设定要舍弃区域的宽高值。应用DropBlock的过程大致如下：（1）在特征图上按照dropout的方式以舍弃概率$\gamma$生成掩模， （2）将掩模中为零的特征单元拓展成以其为中心长宽均为$\text{block_size}$的正方形区域。</p><p> <img src="https://i.postimg.cc/vZYsh6Pb/1541236515330.png" alt="DropBlock算法伪代码"></p><p>论文中表明：为了使拓展后的区域不超出特征图，需要对生成掩模的区域进行限制，如下图所示我们限制掩模仅在绿色方框内生成，应用DropBlock后掩模拓展到了特征图边缘，舍弃部分始终不超出边界。</p><p><img src="https://i.postimg.cc/43MRX2Xx/1541236820742.png" alt="在绿色区域生成掩模，防止拓展后掩模超出范围"></p><p>实际上我觉得这种限制并不必要，如果不加以限制无非是需要使用Clip删除超出范围的掩模。</p><p>DropBlock和Dropout、SpatialDropout的关系也显而易见。当$\text{block_size}=1$时DropBlock就变成了普通的Dropout；当$\text{block_size}$覆盖整个特征图的时候，DropBlock就模拟了SpatialDropout。</p><p>在DropBlock算法中概率$\gamma$是一个比较特殊的变量，论文中将它设置成一个自适应的变量：<br>$$<br>\gamma = \frac{1-\text{keep-prob}}{\text{block_size}^2}\frac{\text{feat_size}^2}{(\text{feat_size-block_size+1})^2}<br>$$<br>其中$1-\text{keep_prob}$意义和传统Dropout中的超参相同，表示每个特征单元被舍弃的概率，在使用DropBlock时我们一样设置这个超参，然后用上式计算进行掩模采样的概率$\gamma$. 如此设置$\gamma$的意义可以这么理解：</p><ul><li>既然只能在$(\text{feat_size-block_size+1})^2$的区域内生成初始掩模，那么每个特征单元被置零的概率等于$ p_i = \gamma * \frac{(\text{feat_size-block_size+1})^2}{\text{feat_size}^2}$</li><li>DropBlock将一个掩模位置拓展成宽高为$\text{block_size}$的正方形区域，拓展后每个特征单元被置零的概率近似等于 $ p_j = p_i * \text{block_size}^2$</li></ul><p>综合起来最终每个特征单元被置零的概率仍然是$ 1-\text{keep-prob} $ , 和Dropout一致。<strong>当然$\gamma$只是一个近似值</strong>。</p><h3 id="四、-实验部分"><a href="#四、-实验部分" class="headerlink" title="四、 实验部分"></a>四、 实验部分</h3><p>作者将DropBlock策略应用到分类、物体检测和分割任务上，都获得了一定程度的提高。</p><h4 id="4-1-分类"><a href="#4-1-分类" class="headerlink" title="4.1 分类"></a>4.1 分类</h4><p>分类任务的主体网络结构是Resnet-50，使用了常规的水平反转、尺度变换等数据增强手段，batch_size设置为1024，训练epoch由官方90调整到270，并且lr 学习率分别在 epoch等于125, 200 和 250 的时候缩小十倍（x0.1）.</p><p><img src="https://i.postimg.cc/8cg86FJ8/1541600358938.png" alt="ResNet-50分类performance对比"></p><p>上表分别是使用不同正则约束方式下性能的对比，官方原始ResNet-50没有使用Dropout正则约束即表格中第一行，Dropout 和 SpatialDropout的结果取值不同 <em>keep_prob</em> 下的最优结果，DropBlock结果是三次训练的平均结果， <em>block_size</em> 设为7. DropBlock对模型性能的提升显而易见。</p><p>为了探究DropBlock使用的位置对模型性能的影响，实验尝试在ResNet-50 的第四组和第三组中使用DropBlock。关于ResNet-50网络结构分组的概念请看原文。在分别尝试在第四组中使用DropBlock，以及在第三（$\gamma$ 缩小四倍）、四组中均使用DropBlock后得出如下结果（on validation）：</p><p><img src="https://i.postimg.cc/ncZbVDfB/1541603203018.png" alt="不同位置和不同设置下使用DropBlock的结果对比"></p><p>第一行表示只在第四组中应用DropBlock，第二行表示同时在第三、四组中使用DropBlock；第一列表示使用固定的 $\gamma$ 且不在残差网络的skip connection 后使用DropBlock， 第二列表示使用固定的 $\gamma$ 但残差网络的skip connection 后使用DropBlock，第三列表示使用所提出的自适应方法计算$\gamma$ 并在残差网络的skip connection 后使用DropBlock。可得出如下结论：</p><ol><li>在更多的地方使用DropBlock对提高模型的性能有好处，组3&amp;4优于组4，on skip connection 优于 no skip connection</li><li>较大的 <em>block_size</em> 效果总比 <em>block_size=1</em> 好，即比用Dropout好</li><li>使用DropBlock的模型在 <em>block_size=7</em> 时效果最佳</li></ol><p>同时在组三、四中使用DropBlock时，<em>实验中将组三的 $\gamma$缩小了4倍</em>，即 $\gamma_3 = \frac{\gamma_4}{4}$， 这样做能保证组三使用较小的舍弃率舍弃较少的激活值，至于 <strong>为什么要这么做以及不这么做会怎么样论文里并没有讨论</strong>。</p><p>实验还针对不同的keep_prob做了对比，设置中我们在组三、四中均使用DropBlock（注意$\gamma$调整），<em>block_size</em> 设为 7， 对比结果如下 (on validation)：</p><p><img src="https://i.postimg.cc/XYHTg1Gk/1541605394805.png" alt="不同keep_prob对比实验"></p><p>曲线说明：1）DropBlock在大多数keep_prob下都能提高模型性能，而且对keep_prob的变化更加鲁棒， 2）使用自适应$\gamma$ 的DropBlock在不同的keep_prob下对模型性能的影响一致。</p><h4 id="4-2-物体检测"><a href="#4-2-物体检测" class="headerlink" title="4.2 物体检测"></a>4.2 物体检测</h4><p>作者将DropBlock用于检测任务中，使用的主体网络时RetinaNet，从任意初始状态开始训练（即train from stratch），看在是否使用DropBlock以及使用多大的 block_size 对检测器训练的影响。图片先缩放到 512-768 左右大小，然后crop或pad 到长边为640大小。网络的每个卷积层后都使用了batch normalization，包括检测器的分类/回归分支。Batch size 大小设为64， 训练了150个epoch。初始lr设置为0.08，然后依次在120和140 eopch缩小10倍。下表是在COCO train2017 上训练，在COCO val2017上evaluation的结果：</p><p><img src="https://i.postimg.cc/mkyn1mWM/1541607155056.png" alt="不同设置下检测器性能比较"></p><p>keep_prob使用了分类任务中发现表现最好的 0.9，同样是从随机初始化开始训练的情况下，使用DropBlock能得到更好的检测器。注意到这里block_size变化对检测器的影响很微妙，但论文里没有详细分析，值得研究。论文从表中的数据得出使用正则约束是从零开始训练检测器很重要的一环，但这个结论有些牵强，毕竟训练的时间差别比较大 — 从Imagenet finetune时只训练了28个epoch而已。</p><h4 id="4-3-PASCAL-VOC-分割"><a href="#4-3-PASCAL-VOC-分割" class="headerlink" title="4.3 PASCAL VOC 分割"></a>4.3 PASCAL VOC 分割</h4><p>在分割任务上的实验与检测上的区别不大，使用的时PASCAL VOC12 数据集（10582 train， 1449 test），主体网络使用开源RetinaNet实现。从随机初始状态开始训练500 epoch 的结果与从Imagenet finetune 45 个epoch的结果相去甚远，但比同是从随机状态开始训练的不使用DropBlock的网络要好很多。</p><p><img src="https://i.postimg.cc/y6mtG83d/1541612036335.png" alt="不同设置下分割任务表现"></p><h3 id="五、结论"><a href="#五、结论" class="headerlink" title="五、结论"></a>五、结论</h3><p>DropBlock有比Dropout更好的正则约束能力，使用DropBlock训练的模型性能更好、更鲁棒。 原因是卷积网络的激活输出连续区域具有很强的关联性，这种关联性通过Dropout随机舍弃不易打破，尽管我们能通过Dropout舍弃一些点但信息仍然通过相邻点传递到下一层，即Dropout阻断或屏蔽语义信息的效率不高；而DropBlock刚好弥补了这方面的不足，通过舍弃一个点以及周围特定区域的所有点，DropBlock能有效屏蔽该区域的语义信息，进而迫使网络从别的区域学习特征（原文：learning multiple discriminative regions instead of only focusing on one discriminative region），网络从全图找信息，也正因为此使用DropBlock的模型对keep_prob表现出更好的鲁棒性。最后用下面这张图来证明DropBlock能从全图找信息，学习到spatially distributed representations</p><p><img src="https://i.postimg.cc/MpFFBcNs/1541613243691.png" alt="DropBlock learns spatially distributed representations"></p><p>第2，3，4行分别是在 <strong>分类任务下</strong> 不使用DropBlock，使用 <em>block_size=1</em> 的DropBlock 和使用 <em>block_size=7</em> 的DropBlock时 ResNet-50 <code>conv5_3</code> 的类激活可视化（Class activation maps：CAM）。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol><li><a href="https://www.jianshu.com/p/b0a328726681">maxout network的TensorFlow实现</a></li><li><a href="https://zhuanlan.zhihu.com/p/31685211">《Maxout Networks》论文阅读</a></li><li><a href="https://www.groundai.com/project/efficient-object-localization-using-convolutional-networks/">Efficient Object Localization Using Convolutional Networks</a></li></ol>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;h2 id=&quot;DropBlock-面向卷积网络的新正则化方法&quot;&gt;&lt;a href=&quot;#DropBlock-面向卷积网络的新正则化方法&quot; class=&quot;headerlink&quot; title=&quot;DropBlock: 面向卷积网络的新正则化方法&quot;&gt;&lt;/a&gt;DropBlock:</summary>
        
      
    
    
    
    <category term="Reading" scheme="https://ukeyboard.github.io/categories/Reading/"/>
    
    
    <category term="Convolutional Network" scheme="https://ukeyboard.github.io/tags/Convolutional-Network/"/>
    
    <category term="Regularization" scheme="https://ukeyboard.github.io/tags/Regularization/"/>
    
  </entry>
  
</feed>
